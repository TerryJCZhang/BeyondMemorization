{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport asyncio\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional\nimport pandas as pd\nfrom tqdm import tqdm\nimport openai\nfrom datasets import load_dataset\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport logging\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass LiveCodeBenchEvaluator:\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initialize the evaluator with OpenAI API key.\n        \n        Args:\n            api_key: OpenAI API key\n        \"\"\"\n        self.client = openai.OpenAI(api_key=api_key)\n        self.dataset = None\n        self.perturbed_problems = []\n        self.results = defaultdict(lambda: defaultdict(list))\n        \n    def load_livecodebench(self):\n        \"\"\"Load the LiveCodeBench dataset.\"\"\"\n        try:\n            # Load LiveCodeBench dataset from HuggingFace\n            self.dataset = load_dataset(\"livecodebench/livecodebench\", trust_remote_code=True)\n            logger.info(f\"Loaded LiveCodeBench with {len(self.dataset['test'])} problems\")\n        except Exception as e:\n            logger.error(f\"Error loading LiveCodeBench: {e}\")\n            # Fallback: create sample data structure if dataset not available\n            logger.info(\"Creating sample data structure for demonstration\")\n            self.dataset = self._create_sample_dataset()\n    \n    def _create_sample_dataset(self):\n        \"\"\"Create a sample dataset structure for demonstration.\"\"\"\n        # This is a fallback for when the actual dataset isn't available\n        sample_problems = []\n        months = pd.date_range('2023-05', '2024-02', freq='MS')\n        \n        for i, month in enumerate(months):\n            for j in range(5):  # 5 problems per month for demo\n                sample_problems.append({\n                    'problem_id': f'problem_{i}_{j}',\n                    'problem': f'Write a function that calculates the sum of {j+1} and {i+1}',\n                    'solution': f'def solution(a={j+1}, b={i+1}): return a + b',\n                    'test_cases': [\n                        {'input': f'{j+1}, {i+1}', 'output': str(j+i+2)}\n                    ],\n                    'date': month.strftime('%Y-%m-%d'),\n                    'difficulty': 'easy'\n                })\n        \n        return {'test': sample_problems}\n    \n    def perturb_problem(self, problem: str, problem_id: str) -> str:\n        \"\"\"\n        Perturb a single problem using OpenAI's O1 model.\n        \n        Args:\n            problem: Original problem statement\n            problem_id: Problem identifier\n            \n        Returns:\n            Perturbed problem statement\n        \"\"\"\n        try:\n            prompt = f\"\"\"You are tasked with creating a variation of the following programming problem. \nThe variation should:\n1. Maintain the same core algorithmic concept\n2. Change surface details (variable names, context, specific values)\n3. Keep similar difficulty level\n4. Be solvable using similar approaches\n\nOriginal Problem:\n{problem}\n\nPlease provide a perturbed version of this problem that tests the same skills but with different details.\"\"\"\n\n            response = self.client.chat.completions.create(\n                model=\"o1-preview\",  # Using O1 model for perturbation\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.7,\n                max_tokens=1000\n            )\n            \n            perturbed = response.choices[0].message.content\n            logger.debug(f\"Perturbed problem {problem_id}\")\n            return perturbed\n            \n        except Exception as e:\n            logger.error(f\"Error perturbing problem {problem_id}: {e}\")\n            # Return original if perturbation fails\n            return problem\n    \n    def perturb_all_problems(self, sample_size: Optional[int] = None):\n        \"\"\"\n        Perturb all problems in the dataset using O1.\n        \n        Args:\n            sample_size: Optional limit on number of problems to perturb\n        \"\"\"\n        problems = self.dataset['test']\n        if sample_size:\n            problems = problems[:sample_size]\n        \n        logger.info(f\"Perturbing {len(problems)} problems...\")\n        \n        with ThreadPoolExecutor(max_workers=5) as executor:\n            futures = []\n            for problem in problems:\n                future = executor.submit(\n                    self.perturb_problem, \n                    problem['problem'], \n                    problem['problem_id']\n                )\n                futures.append((future, problem))\n            \n            for future, original_problem in tqdm(futures, desc=\"Perturbing\"):\n                perturbed_text = future.result()\n                self.perturbed_problems.append({\n                    **original_problem,\n                    'perturbed_problem': perturbed_text\n                })\n    \n    def solve_problem(self, problem: str, model: str) -> str:\n        \"\"\"\n        Solve a problem using specified GPT model.\n        \n        Args:\n            problem: Problem statement\n            model: Model name ('gpt-4' or 'gpt-4o')\n            \n        Returns:\n            Generated solution code\n        \"\"\"\n        try:\n            # Map model names to actual OpenAI model identifiers\n            model_mapping = {\n                'gpt-4': 'gpt-4-turbo-preview',\n                'gpt-4o': 'gpt-4o'\n            }\n            \n            actual_model = model_mapping.get(model, model)\n            \n            prompt = f\"\"\"Solve the following programming problem. Provide only the Python code solution without explanation.\n\nProblem:\n{problem}\n\nSolution:\"\"\"\n\n            response = self.client.chat.completions.create(\n                model=actual_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert programmer. Provide clean, efficient Python code solutions.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.2,\n                max_tokens=1500\n            )\n            \n            solution = response.choices[0].message.content\n            # Extract code from markdown if present\n            if '```python' in solution:\n                solution = solution.split('```python')[1].split('```')[0]\n            elif '```' in solution:\n                solution = solution.split('```')[1].split('```')[0]\n            \n            return solution.strip()\n            \n        except Exception as e:\n            logger.error(f\"Error solving problem with {model}: {e}\")\n            return \"\"\n    \n    def evaluate_solution(self, solution: str, test_cases: List[Dict]) -> bool:\n        \"\"\"\n        Evaluate a solution against test cases.\n        \n        Args:\n            solution: Generated solution code\n            test_cases: List of test cases\n            \n        Returns:\n            Boolean indicating if all test cases passed\n        \"\"\"\n        if not solution:\n            return False\n        \n        try:\n            # Create a safe execution environment\n            exec_globals = {}\n            exec(solution, exec_globals)\n            \n            # Run test cases\n            for test_case in test_cases:\n                try:\n                    # This is simplified - actual evaluation would be more complex\n                    # You'd need to parse inputs/outputs and call the function properly\n                    result = eval(f\"solution({test_case['input']})\", exec_globals)\n                    if str(result) != test_case['output']:\n                        return False\n                except:\n                    return False\n            \n            return True\n            \n        except Exception as e:\n            logger.debug(f\"Solution execution failed: {e}\")\n            return False\n    \n    def run_evaluation(self, models: List[str] = ['gpt-4', 'gpt-4o'], \n                       sample_size: Optional[int] = None):\n        \"\"\"\n        Run the complete evaluation pipeline.\n        \n        Args:\n            models: List of models to evaluate\n            sample_size: Optional limit on number of problems to evaluate\n        \"\"\"\n        # Load dataset if not already loaded\n        if self.dataset is None:\n            self.load_livecodebench()\n        \n        # Perturb problems if not already done\n        if not self.perturbed_problems:\n            self.perturb_all_problems(sample_size)\n        \n        # Evaluate each model on perturbed problems\n        for model in models:\n            logger.info(f\"Evaluating {model}...\")\n            \n            for problem in tqdm(self.perturbed_problems, desc=f\"Evaluating {model}\"):\n                # Get problem date and extract month-year\n                problem_date = datetime.strptime(problem['date'], '%Y-%m-%d')\n                month_key = problem_date.strftime('%Y-%m')\n                \n                # Only process if within our date range\n                if '2023-05' <= month_key <= '2024-02':\n                    # Solve the perturbed problem\n                    solution = self.solve_problem(problem['perturbed_problem'], model)\n                    \n                    # Evaluate the solution\n                    is_correct = self.evaluate_solution(solution, problem['test_cases'])\n                    \n                    # Store results\n                    self.results[model][month_key].append({\n                        'problem_id': problem['problem_id'],\n                        'correct': is_correct,\n                        'solution': solution\n                    })\n                    \n                    # Rate limiting\n                    time.sleep(0.5)\n    \n    def calculate_accuracy(self) -> pd.DataFrame:\n        \"\"\"\n        Calculate accuracy for each model by month.\n        \n        Returns:\n            DataFrame with accuracy metrics\n        \"\"\"\n        accuracy_data = []\n        \n        for model in self.results:\n            for month in self.results[model]:\n                month_results = self.results[model][month]\n                if month_results:\n                    accuracy = sum(r['correct'] for r in month_results) / len(month_results)\n                    accuracy_data.append({\n                        'model': model,\n                        'month': month,\n                        'accuracy': accuracy,\n                        'total_problems': len(month_results),\n                        'correct_problems': sum(r['correct'] for r in month_results)\n                    })\n        \n        df = pd.DataFrame(accuracy_data)\n        if not df.empty:\n            df['month'] = pd.to_datetime(df['month'])\n            df = df.sort_values(['model', 'month'])\n        \n        return df\n    \n    def save_results(self, filepath: str = 'evaluation_results.json'):\n        \"\"\"\n        Save evaluation results to file.\n        \n        Args:\n            filepath: Path to save results\n        \"\"\"\n        results_to_save = {\n            'results': {\n                model: {\n                    month: [\n                        {\n                            'problem_id': r['problem_id'],\n                            'correct': r['correct']\n                        }\n                        for r in results\n                    ]\n                    for month, results in month_results.items()\n                }\n                for model, month_results in self.results.items()\n            },\n            'summary': self.calculate_accuracy().to_dict('records')\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_to_save, f, indent=2, default=str)\n        \n        logger.info(f\"Results saved to {filepath}\")\n    \n    def plot_results(self):\n        \"\"\"Generate and display accuracy plots.\"\"\"\n        import matplotlib.pyplot as plt\n        \n        df = self.calculate_accuracy()\n        \n        if df.empty:\n            logger.warning(\"No results to plot\")\n            return\n        \n        fig, ax = plt.subplots(figsize=(12, 6))\n        \n        for model in df['model'].unique():\n            model_data = df[df['model'] == model]\n            ax.plot(model_data['month'], model_data['accuracy'], \n                   marker='o', label=model, linewidth=2)\n        \n        ax.set_xlabel('Month')\n        ax.set_ylabel('Accuracy')\n        ax.set_title('Model Accuracy on Perturbed LiveCodeBench Problems (May 2023 - Feb 2024)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # Format x-axis\n        import matplotlib.dates as mdates\n        ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n        ax.xaxis.set_major_locator(mdates.MonthLocator())\n        plt.xticks(rotation=45)\n        \n        plt.tight_layout()\n        plt.savefig('accuracy_plot.png', dpi=150)\n        plt.show()\n        \n        logger.info(\"Plot saved as accuracy_plot.png\")\n\n\ndef main():\n    # Configuration\n    API_KEY = os.environ.get('OPENAI_API_KEY')\n    \n    if API_KEY == 'your-api-key-here':\n        logger.error(\"Please set your OpenAI API key in the OPENAI_API_KEY environment variable\")\n        return\n    \n    # Initialize evaluator\n    evaluator = LiveCodeBenchEvaluator(API_KEY)\n    \n    # Run evaluation\n    # Note: Use sample_size parameter to limit the number of problems (for testing/cost management)\n    evaluator.run_evaluation(\n        models=['gpt-4', 'gpt-4o'],\n        sample_size=10  # Limit to 10 problems for demonstration; remove for full evaluation\n    )\n    \n    # Calculate and display accuracy\n    accuracy_df = evaluator.calculate_accuracy()\n    print(\"\\n=== Accuracy Results by Month ===\")\n    print(accuracy_df.to_string())\n    \n    # Save results\n    evaluator.save_results()\n    \n    # Plot results\n    try:\n        evaluator.plot_results()\n    except ImportError:\n        logger.warning(\"Matplotlib not available for plotting\")\n    \n    # Print summary statistics\n    print(\"\\n=== Summary Statistics ===\")\n    for model in accuracy_df['model'].unique():\n        model_data = accuracy_df[accuracy_df['model'] == model]\n        print(f\"\\n{model}:\")\n        print(f\"  Average accuracy: {model_data['accuracy'].mean():.2%}\")\n        print(f\"  Min accuracy: {model_data['accuracy'].min():.2%}\")\n        print(f\"  Max accuracy: {model_data['accuracy'].max():.2%}\")\n        print(f\"  Total problems evaluated: {model_data['total_problems'].sum()}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport ast\nimport asyncio\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional, Any\nimport pandas as pd\nfrom tqdm import tqdm\nimport openai\nfrom datasets import load_dataset\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport logging\nimport subprocess\nimport tempfile\nimport re\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass LiveCodeBenchEvaluator:\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initialize the evaluator with OpenAI API key.\n        \n        Args:\n            api_key: OpenAI API key\n        \"\"\"\n        self.client = openai.OpenAI(api_key=api_key)\n        self.dataset = None\n        self.perturbed_problems = []\n        self.results = defaultdict(lambda: defaultdict(list))\n        self.detailed_results = []  # Store everything for debugging\n        \n    def load_livecodebench(self):\n        \"\"\"Load the LiveCodeBench dataset.\"\"\"\n        try:\n            # Load LiveCodeBench dataset with correct path and version\n            # release_v1 contains problems from May 2023 to Mar 2024 (400 problems)\n            # which covers our target range\n            self.dataset = load_dataset(\n                \"livecodebench/code_generation_lite\", \n                version_tag=\"release_v1\",\n                trust_remote_code=True,\n                split=\"test\"\n            )\n            logger.info(f\"Loaded LiveCodeBench with {len(self.dataset)} problems\")\n            \n            # Filter for date range May 2023 - Feb 2024\n            filtered_data = []\n            for item in self.dataset:\n                # LiveCodeBench has 'question_date' field\n                date_field = item.get('question_date', item.get('date', item.get('created_at', '')))\n                if date_field:\n                    # Parse date and check range\n                    try:\n                        if isinstance(date_field, str):\n                            problem_date = datetime.strptime(date_field[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_field)\n                            \n                        if datetime(2023, 5, 1) <= problem_date <= datetime(2024, 2, 29):\n                            filtered_data.append(item)\n                    except Exception as e:\n                        logger.debug(f\"Could not parse date {date_field}: {e}\")\n                        # Include if we can't parse date\n                        filtered_data.append(item)\n            \n            if not filtered_data:\n                # If no filtering worked, use all data\n                logger.warning(\"Date filtering didn't work, using all problems\")\n                filtered_data = list(self.dataset)\n            \n            self.dataset = filtered_data\n            logger.info(f\"Using {len(self.dataset)} problems\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading LiveCodeBench: {e}\")\n            logger.info(\"Creating sample dataset for demonstration\")\n            self.dataset = self._create_sample_dataset()\n    \n    def _create_sample_dataset(self):\n        \"\"\"Create a sample dataset structure for demonstration if real dataset fails.\"\"\"\n        sample_problems = []\n        months = pd.date_range('2023-05', '2024-02', freq='MS')\n        \n        for i, month in enumerate(months):\n            for j in range(3):  # 3 problems per month for demo\n                sample_problems.append({\n                    'question_id': f'problem_{i}_{j}',\n                    'question_date': month.strftime('%Y-%m-%d'),\n                    'question_title': f'Two Sum Variation {i}_{j}',\n                    'question_content': f'''Given an array of integers nums and an integer target, \n                    return indices of the two numbers such that they add up to target.\n                    Example: nums = [{j}, {i}, {j+i}], target = {j+i}\n                    Output: [0, 1]''',\n                    'starter_code': 'def twoSum(nums, target):\\n    pass',\n                    'public_test_cases': [\n                        {'input': f'[{j}, {i}, {j+i}], {j+i}', 'expected_output': '[0, 1]'},\n                        {'input': f'[1, 2, 3], 5', 'expected_output': '[1, 2]'}\n                    ],\n                    'private_test_cases': [\n                        {'input': '[2, 7, 11, 15], 9', 'expected_output': '[0, 1]'}\n                    ],\n                    'difficulty': 'easy',\n                    'metadata': {'source': 'demo'}\n                })\n        \n        return sample_problems\n    \n    def extract_code_from_response(self, response: str) -> str:\n        \"\"\"Extract code from model response.\"\"\"\n        # Try to find code blocks\n        if '```python' in response:\n            matches = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        elif '```' in response:\n            matches = re.findall(r'```\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        \n        # Try to find function definition\n        if 'def ' in response:\n            # Extract from first def to the end\n            lines = response.split('\\n')\n            code_lines = []\n            in_function = False\n            for line in lines:\n                if 'def ' in line:\n                    in_function = True\n                if in_function:\n                    code_lines.append(line)\n            return '\\n'.join(code_lines)\n        \n        return response.strip()\n    \n    def perturb_problem(self, problem_data: Dict) -> Dict:\n        \"\"\"\n        Perturb a single problem using OpenAI's O1 model.\n        \n        Args:\n            problem_data: Original problem data\n            \n        Returns:\n            Problem data with perturbed version added\n        \"\"\"\n        try:\n            # Extract problem description - LiveCodeBench uses 'question_content'\n            problem_text = problem_data.get('question_content', \n                                           problem_data.get('problem_description', \n                                           problem_data.get('problem', '')))\n            \n            prompt = f\"\"\"You are tasked with creating a variation of the following programming problem.\nThe variation should:\n1. Keep the exact same algorithmic approach and complexity\n2. Change variable names, function names, and context\n3. Modify specific numbers and test cases\n4. Maintain the same difficulty level\n\nOriginal Problem:\n{problem_text}\n\nProvide ONLY the perturbed problem statement, no explanations.\"\"\"\n\n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=\"o1-preview\",  # Using O1-preview model\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_completion_tokens=4096  # O1 uses max_completion_tokens\n            )\n            \n            elapsed = time.time() - start_time\n            logger.info(f\"O1 perturbation took {elapsed:.1f} seconds\")\n            \n            perturbed = response.choices[0].message.content.strip()\n            \n            return {\n                **problem_data,\n                'perturbed_problem': perturbed,\n                'perturbation_time': elapsed\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error perturbing problem: {e}\")\n            # Return original if perturbation fails\n            return {\n                **problem_data,\n                'perturbed_problem': problem_text,\n                'perturbation_error': str(e)\n            }\n    \n    def perturb_all_problems(self, sample_size: Optional[int] = None):\n        \"\"\"\n        Perturb all problems in the dataset using O1.\n        \n        Args:\n            sample_size: Optional limit on number of problems to perturb\n        \"\"\"\n        problems = self.dataset\n        if sample_size:\n            problems = problems[:sample_size]\n        \n        logger.info(f\"Perturbing {len(problems)} problems with O1...\")\n        \n        for problem in tqdm(problems, desc=\"Perturbing with O1\"):\n            perturbed = self.perturb_problem(problem)\n            self.perturbed_problems.append(perturbed)\n            \n            # O1 has strict rate limits, add delay\n            time.sleep(2)  # Adjust based on your rate limits\n    \n    def solve_problem(self, problem_text: str, model: str, original_problem_data: Dict) -> Tuple[str, str, float]:\n        \"\"\"\n        Solve a problem using specified GPT model.\n        \n        Args:\n            problem_text: Problem statement (perturbed)\n            model: Model name ('gpt-4' or 'gpt-4o')\n            original_problem_data: Original problem data for function signature\n            \n        Returns:\n            Tuple of (generated solution code, raw response, time taken)\n        \"\"\"\n        try:\n            # Extract function signature from LiveCodeBench data\n            starter_code = original_problem_data.get('starter_code', '')\n            entry_point = original_problem_data.get('entry_point', '')\n            \n            # Build the prompt parts separately to avoid backslash in f-string\n            prompt_parts = [\n                \"Solve the following programming problem.\",\n                \"Provide a complete, working Python solution.\",\n                \"\",\n                \"Problem:\",\n                problem_text,\n                \"\"\n            ]\n            \n            # Add function signature info if available\n            if starter_code:\n                prompt_parts.append(f\"Use this function signature:\")\n                prompt_parts.append(starter_code)\n            elif entry_point:\n                prompt_parts.append(f\"The main function should be named: {entry_point}\")\n            \n            prompt_parts.append(\"\")\n            prompt_parts.append(\"Provide the complete Python code solution. Do not include any explanations, just the code:\")\n            \n            # Join all parts with newline\n            prompt = '\\n'.join(prompt_parts)\n\n            # Map model names to actual OpenAI model identifiers\n            model_mapping = {\n                'gpt-4': 'gpt-4-turbo',  # Latest GPT-4 Turbo\n                'gpt-4o': 'gpt-4o'  # GPT-4o\n            }\n            \n            actual_model = model_mapping.get(model, model)\n            \n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=actual_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert competitive programmer. Provide complete, efficient, and correct Python code solutions. Output only code, no explanations.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.0,  # Use 0 for deterministic output\n                max_tokens=4096  # Much higher limit for complex solutions\n            )\n            \n            elapsed = time.time() - start_time\n            logger.debug(f\"{model} took {elapsed:.1f} seconds\")\n            \n            raw_response = response.choices[0].message.content\n            solution = self.extract_code_from_response(raw_response)\n            \n            return solution, raw_response, elapsed\n            \n        except Exception as e:\n            logger.error(f\"Error solving problem with {model}: {e}\")\n            return \"\", f\"Error: {str(e)}\", 0.0\n    \n    def evaluate_solution(self, solution: str, problem_data: Dict) -> Tuple[bool, str, List[Dict]]:\n        \"\"\"\n        Evaluate a solution against test cases from LiveCodeBench.\n        \n        Args:\n            solution: Generated solution code\n            problem_data: Problem data including test cases\n            \n        Returns:\n            Tuple of (success, error_message, test_results)\n        \"\"\"\n        if not solution:\n            return False, \"No solution provided\", []\n        \n        test_results = []\n        \n        try:\n            # LiveCodeBench has public_test_cases and private_test_cases\n            public_tests = problem_data.get('public_test_cases', [])\n            private_tests = problem_data.get('private_test_cases', [])\n            test_cases = problem_data.get('test_cases', [])  # Fallback\n            \n            # Combine all test cases\n            all_tests = public_tests + private_tests + test_cases\n            \n            if not all_tests:\n                logger.warning(\"No test cases found in problem data\")\n                return False, \"No test cases found\", []\n            \n            # Get the function name/entry point\n            entry_point = problem_data.get('entry_point', None)\n            if not entry_point:\n                # Try to extract from starter_code\n                starter_code = problem_data.get('starter_code', '')\n                if 'def ' in starter_code:\n                    entry_point = starter_code.split('def ')[1].split('(')[0].strip()\n                else:\n                    # Try to extract from solution\n                    if 'def ' in solution:\n                        entry_point = solution.split('def ')[1].split('(')[0].strip()\n                    else:\n                        entry_point = 'solution'  # Default\n            \n            # Create temporary file to run the solution\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n                # Write solution\n                f.write(solution + '\\n\\n')\n                \n                # Add test execution code\n                test_code = self.generate_test_code(entry_point, all_tests)\n                f.write(test_code)\n                f.flush()\n                \n                temp_file = f.name\n            \n            # Run the solution with timeout\n            try:\n                result = subprocess.run(\n                    ['python', temp_file],\n                    capture_output=True,\n                    text=True,\n                    timeout=10  # 10 second timeout\n                )\n                \n                # Parse results\n                if result.returncode == 0 and \"PASS\" in result.stdout:\n                    # Count passes\n                    passes = result.stdout.count(\"PASS\")\n                    total = len(all_tests)\n                    passed = passes == total\n                    \n                    test_results = [\n                        {'test': i, 'result': 'PASS' if f\"Test {i}: PASS\" in result.stdout else 'FAIL'} \n                        for i in range(len(all_tests))\n                    ]\n                    \n                    return passed, f\"{passes}/{total} tests passed\", test_results\n                else:\n                    error_msg = result.stderr or result.stdout\n                    test_results = [{'test': i, 'result': 'FAIL', 'error': error_msg[:200]} for i in range(len(all_tests))]\n                    return False, f\"Execution error: {error_msg[:500]}\", test_results\n                    \n            except subprocess.TimeoutExpired:\n                return False, \"Solution timed out\", [{'test': i, 'result': 'TIMEOUT'} for i in range(len(all_tests))]\n            finally:\n                # Clean up temp file\n                try:\n                    os.unlink(temp_file)\n                except:\n                    pass\n                \n        except Exception as e:\n            logger.error(f\"Evaluation error: {e}\\n{traceback.format_exc()}\")\n            return False, f\"Evaluation error: {str(e)}\", []\n    \n    def generate_test_code(self, func_name: str, test_cases: List) -> str:\n        \"\"\"Generate test execution code based on LiveCodeBench structure.\"\"\"\n        test_code = f\"\"\"\n# Test execution\nimport sys\nimport json\ntest_passed = True\ntotal_tests = {len(test_cases)}\npasses = 0\n\ntry:\n\"\"\"\n        \n        for i, test in enumerate(test_cases):\n            if isinstance(test, dict):\n                # LiveCodeBench format typically has 'input' and 'expected_output' or 'output'\n                input_str = test.get('input', test.get('inputs', ''))\n                expected_str = test.get('expected_output', test.get('output', test.get('expected', '')))\n                \n                # Clean up the input string if needed\n                if isinstance(input_str, str):\n                    # Remove any function call wrapping if present\n                    if func_name in input_str:\n                        input_str = input_str.replace(f'{func_name}(', '').rstrip(')')\n                \n                test_code += f\"\"\"\n    # Test {i}\n    try:\n        result = {func_name}({input_str})\n        expected = {expected_str}\n        if result == expected:\n            print(f\"Test {i}: PASS\")\n            passes += 1\n        else:\n            print(f\"Test {i}: FAIL - Expected {{expected}}, got {{result}}\")\n            test_passed = False\n    except Exception as e:\n        print(f\"Test {i}: ERROR - {{e}}\")\n        test_passed = False\n\"\"\"\n            \n        test_code += \"\"\"\n    print(f\"\\\\nTotal: {passes}/{total_tests} tests passed\")\n    if not test_passed:\n        sys.exit(1)\nexcept Exception as e:\n    print(f\"Error running tests: {e}\")\n    import traceback\n    traceback.print_exc()\n    sys.exit(1)\n\"\"\"\n        return test_code\n    \n    def run_evaluation(self, models: List[str] = ['gpt-4', 'gpt-4o'], \n                       sample_size: Optional[int] = None):\n        \"\"\"\n        Run the complete evaluation pipeline.\n        \n        Args:\n            models: List of models to evaluate\n            sample_size: Optional limit on number of problems to evaluate\n        \"\"\"\n        # Load dataset if not already loaded\n        if self.dataset is None:\n            self.load_livecodebench()\n        \n        # Perturb problems if not already done\n        if not self.perturbed_problems:\n            self.perturb_all_problems(sample_size)\n        \n        # Evaluate each model on perturbed problems\n        for model in models:\n            logger.info(f\"Evaluating {model}...\")\n            \n            for problem in tqdm(self.perturbed_problems, desc=f\"Evaluating {model}\"):\n                # Get problem date and extract month-year\n                # LiveCodeBench uses 'question_date' field\n                date_str = problem.get('question_date', problem.get('date', problem.get('created_at', '')))\n                \n                if date_str:\n                    try:\n                        if isinstance(date_str, str):\n                            problem_date = datetime.strptime(date_str[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_str)\n                        month_key = problem_date.strftime('%Y-%m')\n                    except:\n                        month_key = '2023-12'  # Default if parsing fails\n                else:\n                    month_key = '2023-12'  # Default if no date\n                \n                # Get problem ID\n                problem_id = problem.get('question_id', problem.get('id', problem.get('problem_id', 'unknown')))\n                \n                # Solve the perturbed problem\n                solution, raw_response, solve_time = self.solve_problem(\n                    problem['perturbed_problem'], \n                    model,\n                    problem\n                )\n                \n                # Evaluate the solution\n                is_correct, error_msg, test_results = self.evaluate_solution(solution, problem)\n                \n                # Store detailed results\n                detailed_result = {\n                    'problem_id': problem_id,\n                    'model': model,\n                    'month': month_key,\n                    'correct': is_correct,\n                    'solution': solution,\n                    'raw_response': raw_response,\n                    'error_message': error_msg,\n                    'test_results': test_results,\n                    'solve_time': solve_time,\n                    'original_problem': problem.get('question_content', problem.get('problem', '')),\n                    'perturbed_problem': problem['perturbed_problem']\n                }\n                \n                self.detailed_results.append(detailed_result)\n                \n                # Store summary results\n                self.results[model][month_key].append({\n                    'problem_id': problem_id,\n                    'correct': is_correct\n                })\n                \n                # Rate limiting between API calls\n                time.sleep(1)\n    \n    def calculate_accuracy(self) -> pd.DataFrame:\n        \"\"\"\n        Calculate accuracy for each model by month.\n        \n        Returns:\n            DataFrame with accuracy metrics\n        \"\"\"\n        accuracy_data = []\n        \n        for model in self.results:\n            for month in sorted(self.results[model].keys()):\n                month_results = self.results[model][month]\n                if month_results:\n                    accuracy = sum(r['correct'] for r in month_results) / len(month_results)\n                    accuracy_data.append({\n                        'model': model,\n                        'month': month,\n                        'accuracy': accuracy * 100,  # Convert to percentage\n                        'total_problems': len(month_results),\n                        'correct_problems': sum(r['correct'] for r in month_results)\n                    })\n        \n        df = pd.DataFrame(accuracy_data)\n        if not df.empty:\n            df['month'] = pd.to_datetime(df['month'] + '-01')\n            df = df.sort_values(['model', 'month'])\n        \n        return df\n    \n    def save_results(self, filepath: str = 'evaluation_results.json'):\n        \"\"\"\n        Save complete evaluation results including all intermediate outputs.\n        \n        Args:\n            filepath: Path to save results\n        \"\"\"\n        # Calculate summary\n        summary_df = self.calculate_accuracy()\n        \n        results_to_save = {\n            'summary': summary_df.to_dict('records') if not summary_df.empty else [],\n            'detailed_results': self.detailed_results,\n            'metadata': {\n                'evaluation_date': datetime.now().isoformat(),\n                'total_problems': len(self.perturbed_problems),\n                'models_evaluated': list(self.results.keys())\n            }\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_to_save, f, indent=2, default=str)\n        \n        logger.info(f\"Complete results saved to {filepath}\")\n        \n        # Also save a separate debugging file with just the failed cases\n        failed_cases = [r for r in self.detailed_results if not r['correct']]\n        if failed_cases:\n            with open('failed_cases.json', 'w') as f:\n                json.dump(failed_cases, f, indent=2, default=str)\n            logger.info(f\"Failed cases saved to failed_cases.json ({len(failed_cases)} failures)\")\n    \n    def plot_results(self):\n        \"\"\"Generate and display accuracy plots.\"\"\"\n        import matplotlib.pyplot as plt\n        \n        df = self.calculate_accuracy()\n        \n        if df.empty:\n            logger.warning(\"No results to plot\")\n            return\n        \n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n        \n        # Plot 1: Accuracy over time\n        for model in df['model'].unique():\n            model_data = df[df['model'] == model]\n            ax1.plot(model_data['month'], model_data['accuracy'], \n                    marker='o', label=model, linewidth=2, markersize=8)\n        \n        ax1.set_xlabel('Month')\n        ax1.set_ylabel('Accuracy (%)')\n        ax1.set_title('Model Accuracy on Perturbed LiveCodeBench Problems (May 2023 - Feb 2024)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_ylim(0, 100)\n        \n        # Format x-axis\n        import matplotlib.dates as mdates\n        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n        ax1.xaxis.set_major_locator(mdates.MonthLocator())\n        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n        \n        # Plot 2: Bar chart comparing total accuracy\n        model_names = df['model'].unique()\n        avg_accuracies = [df[df['model'] == m]['accuracy'].mean() for m in model_names]\n        \n        bars = ax2.bar(model_names, avg_accuracies, color=['blue', 'green'])\n        ax2.set_ylabel('Average Accuracy (%)')\n        ax2.set_title('Overall Average Accuracy Comparison')\n        ax2.set_ylim(0, 100)\n        \n        # Add value labels on bars\n        for bar, val in zip(bars, avg_accuracies):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{val:.1f}%', ha='center', va='bottom')\n        \n        ax2.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plt.savefig('accuracy_plot.png', dpi=150)\n        plt.show()\n        \n        logger.info(\"Plot saved as accuracy_plot.png\")\n\n\ndef main():\n    # Configuration\n    API_KEY = os.environ.get('OPENAI_API_KEY')\n    \n    if API_KEY == 'your-api-key-here':\n        logger.error(\"Please set your OpenAI API key in the OPENAI_API_KEY environment variable\")\n        return\n    \n    # Initialize evaluator\n    evaluator = LiveCodeBenchEvaluator(API_KEY)\n    \n    # Run evaluation\n    # Start with small sample for testing, then increase\n    evaluator.run_evaluation(\n        models=['gpt-4', 'gpt-4o'],\n        sample_size=5  # Start small for testing, then remove this limit\n    )\n    \n    # Calculate and display accuracy\n    accuracy_df = evaluator.calculate_accuracy()\n    print(\"\\n\" + \"=\"*60)\n    print(\"ACCURACY RESULTS BY MONTH\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            print(f\"\\n{model}:\")\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            for _, row in model_data.iterrows():\n                print(f\"  {row['month'].strftime('%b %Y')}: {row['accuracy']:.1f}% \"\n                      f\"({row['correct_problems']}/{row['total_problems']} correct)\")\n    \n    # Save complete results with debugging info\n    evaluator.save_results()\n    \n    # Plot results\n    try:\n        evaluator.plot_results()\n    except ImportError:\n        logger.warning(\"Matplotlib not available for plotting\")\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY STATISTICS\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            print(f\"\\n{model}:\")\n            print(f\"  Average accuracy: {model_data['accuracy'].mean():.1f}%\")\n            print(f\"  Min accuracy: {model_data['accuracy'].min():.1f}%\")\n            print(f\"  Max accuracy: {model_data['accuracy'].max():.1f}%\")\n            print(f\"  Total problems evaluated: {model_data['total_problems'].sum()}\")\n            print(f\"  Total correct: {model_data['correct_problems'].sum()}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport ast\nimport asyncio\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional, Any\nimport pandas as pd\nfrom tqdm import tqdm\nimport openai\nfrom datasets import load_dataset\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport logging\nimport subprocess\nimport tempfile\nimport re\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass LiveCodeBenchEvaluator:\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initialize the evaluator with OpenAI API key.\n        \n        Args:\n            api_key: OpenAI API key\n        \"\"\"\n        self.client = openai.OpenAI(api_key=api_key)\n        self.dataset = None\n        self.perturbed_problems = []\n        self.results = defaultdict(lambda: defaultdict(list))\n        self.detailed_results = []  # Store everything for debugging\n        \n    def load_livecodebench(self):\n        \"\"\"Load the LiveCodeBench dataset.\"\"\"\n        try:\n            # Load LiveCodeBench dataset with correct path and version\n            # release_v1 contains problems from May 2023 to Mar 2024 (400 problems)\n            # which covers our target range\n            self.dataset = load_dataset(\n                \"livecodebench/code_generation_lite\", \n                version_tag=\"release_v1\",\n                trust_remote_code=True,\n                split=\"test\"\n            )\n            logger.info(f\"Loaded LiveCodeBench with {len(self.dataset)} problems\")\n            \n            # Filter for date range May 2023 - Feb 2024\n            filtered_data = []\n            for item in self.dataset:\n                # LiveCodeBench has 'question_date' field\n                date_field = item.get('question_date', item.get('date', item.get('created_at', '')))\n                if date_field:\n                    # Parse date and check range\n                    try:\n                        if isinstance(date_field, str):\n                            problem_date = datetime.strptime(date_field[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_field)\n                            \n                        if datetime(2023, 5, 1) <= problem_date <= datetime(2024, 2, 29):\n                            filtered_data.append(item)\n                    except Exception as e:\n                        logger.debug(f\"Could not parse date {date_field}: {e}\")\n                        # Include if we can't parse date\n                        filtered_data.append(item)\n            \n            if not filtered_data:\n                # If no filtering worked, use all data\n                logger.warning(\"Date filtering didn't work, using all problems\")\n                filtered_data = list(self.dataset)\n            \n            self.dataset = filtered_data\n            logger.info(f\"Using {len(self.dataset)} problems\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading LiveCodeBench: {e}\")\n            logger.info(\"Creating sample dataset for demonstration\")\n            self.dataset = self._create_sample_dataset()\n    \n    def _create_sample_dataset(self):\n        \"\"\"Create a sample dataset structure for demonstration if real dataset fails.\"\"\"\n        sample_problems = []\n        months = pd.date_range('2023-05', '2024-02', freq='MS')\n        \n        for i, month in enumerate(months):\n            for j in range(3):  # 3 problems per month for demo\n                sample_problems.append({\n                    'question_id': f'problem_{i}_{j}',\n                    'question_date': month.strftime('%Y-%m-%d'),\n                    'question_title': f'Two Sum Variation {i}_{j}',\n                    'question_content': f'''Given an array of integers nums and an integer target, \n                    return indices of the two numbers such that they add up to target.\n                    Example: nums = [{j}, {i}, {j+i}], target = {j+i}\n                    Output: [0, 1]''',\n                    'starter_code': 'def twoSum(nums, target):\\n    pass',\n                    'public_test_cases': [\n                        {'input': f'[{j}, {i}, {j+i}], {j+i}', 'expected_output': '[0, 1]'},\n                        {'input': f'[1, 2, 3], 5', 'expected_output': '[1, 2]'}\n                    ],\n                    'private_test_cases': [\n                        {'input': '[2, 7, 11, 15], 9', 'expected_output': '[0, 1]'}\n                    ],\n                    'difficulty': 'easy',\n                    'metadata': {'source': 'demo'}\n                })\n        \n        return sample_problems\n    \n    def extract_code_from_response(self, response: str) -> str:\n        \"\"\"Extract code from model response.\"\"\"\n        # Try to find code blocks\n        if '```python' in response:\n            matches = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        elif '```' in response:\n            matches = re.findall(r'```\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        \n        # Try to find function definition\n        if 'def ' in response:\n            # Extract from first def to the end\n            lines = response.split('\\n')\n            code_lines = []\n            in_function = False\n            for line in lines:\n                if 'def ' in line:\n                    in_function = True\n                if in_function:\n                    code_lines.append(line)\n            return '\\n'.join(code_lines)\n        \n        return response.strip()\n    \n    def perturb_problem(self, problem_data: Dict) -> Dict:\n        \"\"\"\n        Perturb a single problem using OpenAI's O1 model.\n        \n        Args:\n            problem_data: Original problem data\n            \n        Returns:\n            Problem data with perturbed version added\n        \"\"\"\n        try:\n            # Extract problem description - LiveCodeBench uses 'question_content'\n            problem_text = problem_data.get('question_content', \n                                           problem_data.get('problem_description', \n                                           problem_data.get('problem', '')))\n            \n            prompt = f\"\"\"You are tasked with creating a variation of the following programming problem.\nThe variation should:\n1. Keep the exact same algorithmic approach and complexity\n2. Change variable names, function names, and context\n3. Modify specific numbers and test cases\n4. Maintain the same difficulty level\n\nOriginal Problem:\n{problem_text}\n\nProvide ONLY the perturbed problem statement, no explanations.\"\"\"\n\n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=\"o1\",  # Using O1-preview model\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_completion_tokens=4096  # O1 uses max_completion_tokens\n            )\n            \n            elapsed = time.time() - start_time\n            logger.info(f\"O1 perturbation took {elapsed:.1f} seconds\")\n            \n            perturbed = response.choices[0].message.content.strip()\n            \n            return {\n                **problem_data,\n                'perturbed_problem': perturbed,\n                'perturbation_time': elapsed\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error perturbing problem: {e}\")\n            # Return original if perturbation fails\n            return {\n                **problem_data,\n                'perturbed_problem': problem_text,\n                'perturbation_error': str(e)\n            }\n    \n    def perturb_all_problems(self, sample_size: Optional[int] = None):\n        \"\"\"\n        Perturb all problems in the dataset using O1.\n        \n        Args:\n            sample_size: Optional limit on number of problems to perturb\n        \"\"\"\n        problems = self.dataset\n        if sample_size:\n            problems = problems[:sample_size]\n        \n        logger.info(f\"Perturbing {len(problems)} problems with O1...\")\n        \n        for problem in tqdm(problems, desc=\"Perturbing with O1\"):\n            perturbed = self.perturb_problem(problem)\n            self.perturbed_problems.append(perturbed)\n            \n            # O1 has strict rate limits, add delay\n            time.sleep(2)  # Adjust based on your rate limits\n    \n    def solve_problem(self, problem_text: str, model: str, original_problem_data: Dict) -> Tuple[str, str, float]:\n        \"\"\"\n        Solve a problem using specified GPT model.\n        \n        Args:\n            problem_text: Problem statement (perturbed)\n            model: Model name ('gpt-4' or 'gpt-4o')\n            original_problem_data: Original problem data for function signature\n            \n        Returns:\n            Tuple of (generated solution code, raw response, time taken)\n        \"\"\"\n        try:\n            # Extract function signature from LiveCodeBench data\n            starter_code = original_problem_data.get('starter_code', '')\n            entry_point = original_problem_data.get('entry_point', '')\n            \n            # Build the prompt parts separately to avoid backslash in f-string\n            prompt_parts = [\n                \"Solve the following programming problem.\",\n                \"Provide a complete, working Python solution.\",\n                \"\",\n                \"Problem:\",\n                problem_text,\n                \"\"\n            ]\n            \n            # Add function signature info if available\n            if starter_code:\n                prompt_parts.append(f\"Use this function signature:\")\n                prompt_parts.append(starter_code)\n            elif entry_point:\n                prompt_parts.append(f\"The main function should be named: {entry_point}\")\n            \n            prompt_parts.append(\"\")\n            prompt_parts.append(\"Provide the complete Python code solution. Do not include any explanations, just the code:\")\n            \n            # Join all parts with newline\n            prompt = '\\n'.join(prompt_parts)\n\n            # Map model names to actual OpenAI model identifiers\n            model_mapping = {\n                'gpt-4': 'gpt-4-turbo',  # Latest GPT-4 Turbo\n                'gpt-4o': 'gpt-4o'  # GPT-4o\n            }\n            \n            actual_model = model_mapping.get(model, model)\n            \n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=actual_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert competitive programmer. Provide complete, efficient, and correct Python code solutions. Output only code, no explanations.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.0,  # Use 0 for deterministic output\n                max_tokens=4096  # Much higher limit for complex solutions\n            )\n            \n            elapsed = time.time() - start_time\n            logger.debug(f\"{model} took {elapsed:.1f} seconds\")\n            \n            raw_response = response.choices[0].message.content\n            solution = self.extract_code_from_response(raw_response)\n            \n            return solution, raw_response, elapsed\n            \n        except Exception as e:\n            logger.error(f\"Error solving problem with {model}: {e}\")\n            return \"\", f\"Error: {str(e)}\", 0.0\n    \n    def evaluate_solution(self, solution: str, problem_data: Dict) -> Tuple[bool, str, List[Dict]]:\n        \"\"\"\n        Evaluate a solution against test cases from LiveCodeBench.\n        \n        Args:\n            solution: Generated solution code\n            problem_data: Problem data including test cases\n            \n        Returns:\n            Tuple of (success, error_message, test_results)\n        \"\"\"\n        if not solution:\n            return False, \"No solution provided\", []\n        \n        test_results = []\n        \n        try:\n            # LiveCodeBench has public_test_cases and private_test_cases\n            public_tests = problem_data.get('public_test_cases', [])\n            private_tests = problem_data.get('private_test_cases', [])\n            test_cases = problem_data.get('test_cases', [])  # Fallback\n            \n            # Combine all test cases\n            all_tests = public_tests + private_tests + test_cases\n            \n            if not all_tests:\n                logger.warning(\"No test cases found in problem data\")\n                return False, \"No test cases found\", []\n            \n            # Get the function name/entry point\n            entry_point = problem_data.get('entry_point', None)\n            if not entry_point:\n                # Try to extract from starter_code\n                starter_code = problem_data.get('starter_code', '')\n                if 'def ' in starter_code:\n                    entry_point = starter_code.split('def ')[1].split('(')[0].strip()\n                else:\n                    # Try to extract from solution\n                    if 'def ' in solution:\n                        entry_point = solution.split('def ')[1].split('(')[0].strip()\n                    else:\n                        entry_point = 'solution'  # Default\n            \n            # Create temporary file to run the solution\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n                # Write solution\n                f.write(solution + '\\n\\n')\n                \n                # Add test execution code\n                test_code = self.generate_test_code(entry_point, all_tests)\n                f.write(test_code)\n                f.flush()\n                \n                temp_file = f.name\n            \n            # Run the solution with timeout\n            try:\n                result = subprocess.run(\n                    ['python', temp_file],\n                    capture_output=True,\n                    text=True,\n                    timeout=10  # 10 second timeout\n                )\n                \n                # Parse results\n                if result.returncode == 0 and \"PASS\" in result.stdout:\n                    # Count passes\n                    passes = result.stdout.count(\"PASS\")\n                    total = len(all_tests)\n                    passed = passes == total\n                    \n                    test_results = [\n                        {'test': i, 'result': 'PASS' if f\"Test {i}: PASS\" in result.stdout else 'FAIL'} \n                        for i in range(len(all_tests))\n                    ]\n                    \n                    return passed, f\"{passes}/{total} tests passed\", test_results\n                else:\n                    error_msg = result.stderr or result.stdout\n                    test_results = [{'test': i, 'result': 'FAIL', 'error': error_msg[:200]} for i in range(len(all_tests))]\n                    return False, f\"Execution error: {error_msg[:500]}\", test_results\n                    \n            except subprocess.TimeoutExpired:\n                return False, \"Solution timed out\", [{'test': i, 'result': 'TIMEOUT'} for i in range(len(all_tests))]\n            finally:\n                # Clean up temp file\n                try:\n                    os.unlink(temp_file)\n                except:\n                    pass\n                \n        except Exception as e:\n            logger.error(f\"Evaluation error: {e}\\n{traceback.format_exc()}\")\n            return False, f\"Evaluation error: {str(e)}\", []\n    \n    def generate_test_code(self, func_name: str, test_cases: List) -> str:\n        \"\"\"Generate test execution code based on LiveCodeBench structure.\"\"\"\n        test_code = f\"\"\"\n# Test execution\nimport sys\nimport json\ntest_passed = True\ntotal_tests = {len(test_cases)}\npasses = 0\n\ntry:\n\"\"\"\n        \n        for i, test in enumerate(test_cases):\n            if isinstance(test, dict):\n                # LiveCodeBench format typically has 'input' and 'expected_output' or 'output'\n                input_str = test.get('input', test.get('inputs', ''))\n                expected_str = test.get('expected_output', test.get('output', test.get('expected', '')))\n                \n                # Clean up the input string if needed\n                if isinstance(input_str, str):\n                    # Remove any function call wrapping if present\n                    if func_name in input_str:\n                        input_str = input_str.replace(f'{func_name}(', '').rstrip(')')\n                \n                test_code += f\"\"\"\n    # Test {i}\n    try:\n        result = {func_name}({input_str})\n        expected = {expected_str}\n        if result == expected:\n            print(f\"Test {i}: PASS\")\n            passes += 1\n        else:\n            print(f\"Test {i}: FAIL - Expected {{expected}}, got {{result}}\")\n            test_passed = False\n    except Exception as e:\n        print(f\"Test {i}: ERROR - {{e}}\")\n        test_passed = False\n\"\"\"\n            \n        test_code += \"\"\"\n    print(f\"\\\\nTotal: {passes}/{total_tests} tests passed\")\n    if not test_passed:\n        sys.exit(1)\nexcept Exception as e:\n    print(f\"Error running tests: {e}\")\n    import traceback\n    traceback.print_exc()\n    sys.exit(1)\n\"\"\"\n        return test_code\n    \n    def run_evaluation(self, models: List[str] = ['gpt-4', 'gpt-4o'], \n                       sample_size: Optional[int] = None):\n        \"\"\"\n        Run the complete evaluation pipeline.\n        \n        Args:\n            models: List of models to evaluate\n            sample_size: Optional limit on number of problems to evaluate\n        \"\"\"\n        # Load dataset if not already loaded\n        if self.dataset is None:\n            self.load_livecodebench()\n        \n        # Perturb problems if not already done\n        if not self.perturbed_problems:\n            self.perturb_all_problems(sample_size)\n        \n        # Evaluate each model on perturbed problems\n        for model in models:\n            logger.info(f\"Evaluating {model}...\")\n            \n            for problem in tqdm(self.perturbed_problems, desc=f\"Evaluating {model}\"):\n                # Get problem date and extract month-year\n                # LiveCodeBench uses 'question_date' field\n                date_str = problem.get('question_date', problem.get('date', problem.get('created_at', '')))\n                \n                if date_str:\n                    try:\n                        if isinstance(date_str, str):\n                            problem_date = datetime.strptime(date_str[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_str)\n                        month_key = problem_date.strftime('%Y-%m')\n                    except:\n                        month_key = '2023-12'  # Default if parsing fails\n                else:\n                    month_key = '2023-12'  # Default if no date\n                \n                # Get problem ID\n                problem_id = problem.get('question_id', problem.get('id', problem.get('problem_id', 'unknown')))\n                \n                # Solve the perturbed problem\n                solution, raw_response, solve_time = self.solve_problem(\n                    problem['perturbed_problem'], \n                    model,\n                    problem\n                )\n                \n                # Evaluate the solution\n                is_correct, error_msg, test_results = self.evaluate_solution(solution, problem)\n                \n                # Store detailed results with COMPLETE original problem data\n                detailed_result = {\n                    'problem_id': problem_id,\n                    'model': model,\n                    'month': month_key,\n                    'correct': is_correct,\n                    'solution': solution,\n                    'raw_response': raw_response,\n                    'error_message': error_msg,\n                    'test_results': test_results,\n                    'solve_time': solve_time,\n                    'original_problem_data': {\n                        'question_id': problem.get('question_id'),\n                        'question_date': problem.get('question_date'),\n                        'question_title': problem.get('question_title'),\n                        'question_content': problem.get('question_content', problem.get('problem', '')),\n                        'starter_code': problem.get('starter_code'),\n                        'entry_point': problem.get('entry_point'),\n                        'public_test_cases': problem.get('public_test_cases', []),\n                        'private_test_cases': problem.get('private_test_cases', []),\n                        'test_cases': problem.get('test_cases', []),\n                        'difficulty': problem.get('difficulty'),\n                        'metadata': problem.get('metadata', {})\n                    },\n                    'perturbed_problem': problem['perturbed_problem'],\n                    'perturbation_time': problem.get('perturbation_time'),\n                    'perturbation_error': problem.get('perturbation_error')\n                }\n                \n                self.detailed_results.append(detailed_result)\n                \n                # Store summary results\n                self.results[model][month_key].append({\n                    'problem_id': problem_id,\n                    'correct': is_correct\n                })\n                \n                # Rate limiting between API calls\n                time.sleep(1)\n    \n    def calculate_accuracy(self) -> pd.DataFrame:\n        \"\"\"\n        Calculate accuracy for each model by month.\n        \n        Returns:\n            DataFrame with accuracy metrics\n        \"\"\"\n        accuracy_data = []\n        \n        for model in self.results:\n            for month in sorted(self.results[model].keys()):\n                month_results = self.results[model][month]\n                if month_results:\n                    accuracy = sum(r['correct'] for r in month_results) / len(month_results)\n                    accuracy_data.append({\n                        'model': model,\n                        'month': month,\n                        'accuracy': accuracy * 100,  # Convert to percentage\n                        'total_problems': len(month_results),\n                        'correct_problems': sum(r['correct'] for r in month_results)\n                    })\n        \n        df = pd.DataFrame(accuracy_data)\n        if not df.empty:\n            df['month'] = pd.to_datetime(df['month'] + '-01')\n            df = df.sort_values(['model', 'month'])\n        \n        return df\n    \n    def save_results(self, filepath: str = 'evaluation_results.json'):\n        \"\"\"\n        Save complete evaluation results including all intermediate outputs.\n        \n        Args:\n            filepath: Path to save results\n        \"\"\"\n        # Calculate summary\n        summary_df = self.calculate_accuracy()\n        \n        # Create problems collection with original and perturbed versions\n        problems_collection = []\n        unique_problems = {}\n        \n        # Collect unique problems from detailed results\n        for result in self.detailed_results:\n            problem_id = result['problem_id']\n            if problem_id not in unique_problems:\n                unique_problems[problem_id] = {\n                    'problem_id': problem_id,\n                    'original_problem': result['original_problem_data'],\n                    'perturbed_problem': result['perturbed_problem'],\n                    'perturbation_time': result.get('perturbation_time'),\n                    'perturbation_error': result.get('perturbation_error'),\n                    'solutions': {}\n                }\n            \n            # Add solution for this model\n            model = result['model']\n            unique_problems[problem_id]['solutions'][model] = {\n                'solution_code': result['solution'],\n                'raw_response': result['raw_response'],\n                'correct': result['correct'],\n                'error_message': result['error_message'],\n                'test_results': result['test_results'],\n                'solve_time': result['solve_time']\n            }\n        \n        problems_collection = list(unique_problems.values())\n        \n        results_to_save = {\n            'summary': summary_df.to_dict('records') if not summary_df.empty else [],\n            'problems': problems_collection,\n            'detailed_results': self.detailed_results,\n            'metadata': {\n                'evaluation_date': datetime.now().isoformat(),\n                'total_problems': len(self.perturbed_problems),\n                'models_evaluated': list(self.results.keys())\n            }\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_to_save, f, indent=2, default=str)\n        \n        logger.info(f\"Complete results saved to {filepath}\")\n        \n        # Also save a separate file with just problems and solutions for easy access\n        problems_and_solutions_file = 'problems_and_solutions.json'\n        with open(problems_and_solutions_file, 'w') as f:\n            json.dump({'problems': problems_collection}, f, indent=2, default=str)\n        logger.info(f\"Problems and solutions saved to {problems_and_solutions_file}\")\n        \n        # Also save a separate debugging file with just the failed cases\n        failed_cases = [r for r in self.detailed_results if not r['correct']]\n        if failed_cases:\n            with open('failed_cases.json', 'w') as f:\n                json.dump(failed_cases, f, indent=2, default=str)\n            logger.info(f\"Failed cases saved to failed_cases.json ({len(failed_cases)} failures)\")\n    \n    def plot_results(self):\n        \"\"\"Generate and display accuracy plots.\"\"\"\n        import matplotlib.pyplot as plt\n        \n        df = self.calculate_accuracy()\n        \n        if df.empty:\n            logger.warning(\"No results to plot\")\n            return\n        \n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n        \n        # Plot 1: Accuracy over time\n        for model in df['model'].unique():\n            model_data = df[df['model'] == model]\n            ax1.plot(model_data['month'], model_data['accuracy'], \n                    marker='o', label=model, linewidth=2, markersize=8)\n        \n        ax1.set_xlabel('Month')\n        ax1.set_ylabel('Accuracy (%)')\n        ax1.set_title('Model Accuracy on Perturbed LiveCodeBench Problems (May 2023 - Feb 2024)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_ylim(0, 100)\n        \n        # Format x-axis\n        import matplotlib.dates as mdates\n        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n        ax1.xaxis.set_major_locator(mdates.MonthLocator())\n        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n        \n        # Plot 2: Bar chart comparing total accuracy\n        model_names = df['model'].unique()\n        avg_accuracies = [df[df['model'] == m]['accuracy'].mean() for m in model_names]\n        \n        bars = ax2.bar(model_names, avg_accuracies, color=['blue', 'green'])\n        ax2.set_ylabel('Average Accuracy (%)')\n        ax2.set_title('Overall Average Accuracy Comparison')\n        ax2.set_ylim(0, 100)\n        \n        # Add value labels on bars\n        for bar, val in zip(bars, avg_accuracies):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{val:.1f}%', ha='center', va='bottom')\n        \n        ax2.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plt.savefig('accuracy_plot.png', dpi=150)\n        plt.show()\n        \n        logger.info(\"Plot saved as accuracy_plot.png\")\n\n\ndef main():\n    # Configuration\n    API_KEY = os.environ.get('OPENAI_API_KEY')\n    \n    if API_KEY == 'your-api-key-here':\n        logger.error(\"Please set your OpenAI API key in the OPENAI_API_KEY environment variable\")\n        return\n    \n    # Initialize evaluator\n    evaluator = LiveCodeBenchEvaluator(API_KEY)\n    \n    # Run evaluation\n    # Start with small sample for testing, then increase\n    evaluator.run_evaluation(\n        models=['gpt-4', 'gpt-4o'],\n        sample_size=5  # Start small for testing, then remove this limit\n    )\n    \n    # Calculate and display accuracy\n    accuracy_df = evaluator.calculate_accuracy()\n    print(\"\\n\" + \"=\"*60)\n    print(\"ACCURACY RESULTS BY MONTH\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            print(f\"\\n{model}:\")\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            for _, row in model_data.iterrows():\n                print(f\"  {row['month'].strftime('%b %Y')}: {row['accuracy']:.1f}% \"\n                      f\"({row['correct_problems']}/{row['total_problems']} correct)\")\n    \n    # Save complete results with debugging info\n    evaluator.save_results()\n    \n    # Plot results\n    try:\n        evaluator.plot_results()\n    except ImportError:\n        logger.warning(\"Matplotlib not available for plotting\")\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY STATISTICS\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            print(f\"\\n{model}:\")\n            print(f\"  Average accuracy: {model_data['accuracy'].mean():.1f}%\")\n            print(f\"  Min accuracy: {model_data['accuracy'].min():.1f}%\")\n            print(f\"  Max accuracy: {model_data['accuracy'].max():.1f}%\")\n            print(f\"  Total problems evaluated: {model_data['total_problems'].sum()}\")\n            print(f\"  Total correct: {model_data['correct_problems'].sum()}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport ast\nimport asyncio\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional, Any\nimport pandas as pd\nfrom tqdm import tqdm\nimport openai\nfrom datasets import load_dataset\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport logging\nimport subprocess\nimport tempfile\nimport re\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass LiveCodeBenchEvaluator:\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initialize the evaluator with OpenAI API key.\n        \n        Args:\n            api_key: OpenAI API key\n        \"\"\"\n        self.client = openai.OpenAI(api_key=api_key)\n        self.dataset = None\n        self.perturbed_problems = []\n        self.results = defaultdict(lambda: defaultdict(list))\n        self.detailed_results = []  # Store everything for debugging\n        \n    def load_livecodebench(self):\n        \"\"\"Load the LiveCodeBench dataset.\"\"\"\n        try:\n            # Load LiveCodeBench dataset with correct path and version\n            # release_v1 contains problems from May 2023 to Mar 2024 (400 problems)\n            # which covers our target range\n            self.dataset = load_dataset(\n                \"livecodebench/code_generation_lite\", \n                version_tag=\"release_v1\",\n                trust_remote_code=True,\n                split=\"test\"\n            )\n            logger.info(f\"Loaded LiveCodeBench with {len(self.dataset)} problems\")\n            \n            # Filter for date range May 2023 - Feb 2024\n            filtered_data = []\n            for item in self.dataset:\n                # LiveCodeBench has 'question_date' field\n                date_field = item.get('question_date', item.get('date', item.get('created_at', '')))\n                if date_field:\n                    # Parse date and check range\n                    try:\n                        if isinstance(date_field, str):\n                            problem_date = datetime.strptime(date_field[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_field)\n                            \n                        if datetime(2023, 5, 1) <= problem_date <= datetime(2024, 2, 29):\n                            filtered_data.append(item)\n                    except Exception as e:\n                        logger.debug(f\"Could not parse date {date_field}: {e}\")\n                        # Include if we can't parse date\n                        filtered_data.append(item)\n            \n            if not filtered_data:\n                # If no filtering worked, use all data\n                logger.warning(\"Date filtering didn't work, using all problems\")\n                filtered_data = list(self.dataset)\n            \n            self.dataset = filtered_data\n            logger.info(f\"Using {len(self.dataset)} problems\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading LiveCodeBench: {e}\")\n            logger.info(\"Creating sample dataset for demonstration\")\n            self.dataset = self._create_sample_dataset()\n    \n    def _create_sample_dataset(self):\n        \"\"\"Create a sample dataset structure for demonstration if real dataset fails.\"\"\"\n        sample_problems = []\n        months = pd.date_range('2023-05', '2024-02', freq='MS')\n        \n        for i, month in enumerate(months):\n            for j in range(3):  # 3 problems per month for demo\n                sample_problems.append({\n                    'question_id': f'problem_{i}_{j}',\n                    'question_date': month.strftime('%Y-%m-%d'),\n                    'question_title': f'Two Sum Variation {i}_{j}',\n                    'question_content': f'''Given an array of integers nums and an integer target, \n                    return indices of the two numbers such that they add up to target.\n                    Example: nums = [{j}, {i}, {j+i}], target = {j+i}\n                    Output: [0, 1]''',\n                    'starter_code': 'def twoSum(nums, target):\\n    pass',\n                    'public_test_cases': json.dumps([\n                        {'input': f'[{j}, {i}, {j+i}], {j+i}', 'output': '[0, 1]', 'testtype': 'function'},\n                        {'input': f'[1, 2, 3], 5', 'output': '[1, 2]', 'testtype': 'function'}\n                    ]),\n                    'private_test_cases': json.dumps([\n                        {'input': '[2, 7, 11, 15], 9', 'output': '[0, 1]', 'testtype': 'function'}\n                    ]),\n                    'difficulty': 'easy',\n                    'metadata': json.dumps({'source': 'demo'})\n                })\n        \n        return sample_problems\n    \n    def parse_test_cases(self, test_case_str: str) -> List[Dict]:\n        \"\"\"Parse test cases from JSON string format.\"\"\"\n        if not test_case_str:\n            return []\n        \n        try:\n            if isinstance(test_case_str, str):\n                return json.loads(test_case_str)\n            elif isinstance(test_case_str, list):\n                return test_case_str\n            else:\n                return []\n        except (json.JSONDecodeError, TypeError) as e:\n            logger.warning(f\"Could not parse test cases: {e}\")\n            return []\n    \n    def extract_code_from_response(self, response: str) -> str:\n        \"\"\"Extract code from model response.\"\"\"\n        # Try to find code blocks\n        if '```python' in response:\n            matches = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        elif '```' in response:\n            matches = re.findall(r'```\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        \n        # If no code blocks, return the whole response (it might be just code)\n        return response.strip()\n    \n    def perturb_problem(self, problem_data: Dict) -> Dict:\n        \"\"\"\n        Perturb a single problem using OpenAI's O1 model.\n        \n        Args:\n            problem_data: Original problem data\n            \n        Returns:\n            Problem data with perturbed version added\n        \"\"\"\n        try:\n            # Extract problem description\n            problem_text = problem_data.get('question_content', \n                                           problem_data.get('problem_description', \n                                           problem_data.get('problem', '')))\n            \n            # Parse test cases\n            public_tests = self.parse_test_cases(problem_data.get('public_test_cases', '[]'))\n            private_tests = self.parse_test_cases(problem_data.get('private_test_cases', '[]'))\n            \n            # Create a simpler test case representation for the prompt\n            test_examples = []\n            for test in public_tests: \n                if isinstance(test, dict):\n                    test_examples.append(f\"Input: {test.get('input', '')}\\nOutput: {test.get('output', '')}\")\n            \n            prompt = f\"\"\"You are tasked with creating a variation of the following programming problem.\n\nThe variation should:\n1. Keep the exact same algorithmic approach and complexity\n2. Change variable names, function names, and context (e.g., if it uses 'abc', use something like 'XYZ')\n3. Modify specific values in test cases consistently with the context change\n4. Maintain the same difficulty level and logic\n\nOriginal Problem:\n{problem_text}\n\nOriginal Test Examples:\n{chr(10).join(test_examples)}\n\nProvide the perturbed problem AND perturbed test cases in the following JSON format:\n{{\n    \"problem_statement\": \"...\",\n    \"test_cases\": [\n        {{\"input\": \"...\", \"output\": \"...\", \"testtype\": \"stdin\"}}\n    ]\n}}\n\nMake sure to perturb ALL test values consistently. If the original uses 'abc', 'acb', 'bac', etc., \nand you change to 'XYZ', then use 'XYZ', 'XZY', 'YXZ', etc. correspondingly.\"\"\"\n\n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=\"o1\",  # Using O1-preview model\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_completion_tokens=100000  # O1 uses max_completion_tokens\n            )\n            \n            elapsed = time.time() - start_time\n            logger.info(f\"O1 perturbation took {elapsed:.1f} seconds\")\n            \n            response_text = response.choices[0].message.content.strip()\n            \n            # Try to parse the JSON response\n            try:\n                # Extract JSON from response if wrapped in code blocks\n                if '```json' in response_text:\n                    json_match = re.search(r'```json\\n(.*?)```', response_text, re.DOTALL)\n                    if json_match:\n                        response_text = json_match.group(1)\n                elif '```' in response_text:\n                    json_match = re.search(r'```\\n(.*?)```', response_text, re.DOTALL)\n                    if json_match:\n                        response_text = json_match.group(1)\n                \n                perturbed_data = json.loads(response_text)\n                \n                # Ensure we have the required fields\n                perturbed_problem = perturbed_data.get('problem_statement', response_text)\n                perturbed_test_cases = perturbed_data.get('test_cases', [])\n                \n                # Make sure we have enough perturbed test cases\n                # If not, create modified versions based on the pattern\n                total_tests_needed = len(public_tests) + len(private_tests)\n               # if len(perturbed_test_cases) < total_tests_needed:\n                #    logger.warning(f\"Only got {len(perturbed_test_cases)} perturbed tests, needed {total_tests_needed}\")\n                \n            except (json.JSONDecodeError, Exception) as e:\n                logger.warning(f\"Could not parse perturbed response as JSON: {e}\")\n                # Fall back to using the response as the problem statement\n                perturbed_problem = response_text\n                perturbed_test_cases = []\n                \n                # Try to create perturbed test cases by simple substitution\n                # This is a fallback - ideally O1 should provide them\n                if 'abc' in problem_text.lower() and any('XYZ' in s for s in [response_text]):\n                    # Simple character substitution for test cases\n                    mapping = {'a': 'X', 'b': 'Y', 'c': 'Z'}\n                    for test in public_tests + private_tests:\n                        if isinstance(test, dict):\n                            new_input = test.get('input', '')\n                            new_output = test.get('output', '')\n                            for old, new in mapping.items():\n                                new_input = new_input.replace(old, new)\n                                new_output = new_output.replace(old, new)\n                            perturbed_test_cases.append({\n                                'input': new_input,\n                                'output': new_output,\n                                'testtype': test.get('testtype', 'stdin')\n                            })\n            \n            # Split perturbed test cases back into public and private\n            num_public = len(public_tests)\n            perturbed_public = perturbed_test_cases[:num_public] if perturbed_test_cases else public_tests\n            perturbed_private = perturbed_test_cases[num_public:] if len(perturbed_test_cases) > num_public else private_tests\n            \n            return {\n                **problem_data,\n                'perturbed_problem': perturbed_problem,\n                'perturbed_public_test_cases': perturbed_public,\n                'perturbed_private_test_cases': perturbed_private,\n                'perturbation_time': elapsed\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error perturbing problem: {e}\")\n            # Return original if perturbation fails\n            return {\n                **problem_data,\n                'perturbed_problem': problem_text,\n                'perturbed_public_test_cases': public_tests,\n                'perturbed_private_test_cases': private_tests,\n                'perturbation_error': str(e)\n            }\n    \n    def perturb_all_problems(self, sample_size: Optional[int] = None):\n        \"\"\"\n        Perturb all problems in the dataset using O1.\n        \n        Args:\n            sample_size: Optional limit on number of problems to perturb\n        \"\"\"\n        problems = self.dataset\n        if True:\n            problems = problems\n        \n        logger.info(f\"Perturbing {len(problems)} problems with O1...\")\n        \n        for problem in tqdm(problems, desc=\"Perturbing with O1\"):\n            perturbed = self.perturb_problem(problem)\n            self.perturbed_problems.append(perturbed)\n            \n            # O1 has strict rate limits, add delay\n            time.sleep(2)  # Adjust based on your rate limits\n    \n    def solve_problem(self, problem_text: str, model: str, original_problem_data: Dict) -> Tuple[str, str, float]:\n        \"\"\"\n        Solve a problem using specified GPT model.\n        \n        Args:\n            problem_text: Problem statement (perturbed)\n            model: Model name ('gpt-4' or 'gpt-4o')\n            original_problem_data: Original problem data for function signature\n            \n        Returns:\n            Tuple of (generated solution code, raw response, time taken)\n        \"\"\"\n        try:\n            # Check if this is a stdin/stdout problem or a function problem\n            public_tests = self.parse_test_cases(original_problem_data.get('public_test_cases', '[]'))\n            perturbed_tests = original_problem_data.get('perturbed_public_test_cases', public_tests)\n            \n            is_stdin_problem = False\n            if perturbed_tests and isinstance(perturbed_tests[0], dict):\n                is_stdin_problem = perturbed_tests[0].get('testtype') == 'stdin'\n            \n            # Build the prompt\n            prompt_parts = [\n                \"Solve the following programming problem.\",\n                \"Provide a complete, working Python solution.\",\n                \"\",\n                \"Problem:\",\n                problem_text,\n                \"\"\n            ]\n            \n            # Add guidance based on problem type\n            if is_stdin_problem:\n                prompt_parts.append(\"This is a standard competitive programming problem that reads from stdin and writes to stdout.\")\n                prompt_parts.append(\"Your solution should use input() to read data and print() to output results.\")\n            else:\n                # Extract function signature if available\n                starter_code = original_problem_data.get('starter_code', '')\n                entry_point = original_problem_data.get('entry_point', '')\n                \n                if starter_code:\n                    prompt_parts.append(f\"Use this function signature:\")\n                    prompt_parts.append(starter_code)\n                elif entry_point:\n                    prompt_parts.append(f\"The main function should be named: {entry_point}\")\n            \n            # Add example from perturbed test cases if available\n            if perturbed_tests:\n                prompt_parts.append(\"\\nExample:\")\n                test_example = perturbed_tests[0]\n                if isinstance(test_example, dict):\n                    prompt_parts.append(f\"Input: {test_example.get('input', '')}\")\n                    prompt_parts.append(f\"Output: {test_example.get('output', '')}\")\n            \n            prompt_parts.append(\"\")\n            prompt_parts.append(\"Provide the complete Python code solution. Do not include any explanations, just the code:\")\n            \n            # Join all parts with newline\n            prompt = '\\n'.join(prompt_parts)\n\n            # Map model names to actual OpenAI model identifiers\n            model_mapping = {\n                'gpt-4': 'gpt-4-turbo',  # Latest GPT-4 Turbo\n                'gpt-4o': 'gpt-4o'  # GPT-4o\n            }\n            \n            actual_model = model_mapping.get(model, model)\n            \n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=actual_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert competitive programmer. Provide complete, efficient, and correct Python code solutions. Output only code, no explanations.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.0,  # Use 0 for deterministic output\n                max_tokens=100000  # Much higher limit for complex solutions\n            )\n            \n            elapsed = time.time() - start_time\n            logger.debug(f\"{model} took {elapsed:.1f} seconds\")\n            \n            raw_response = response.choices[0].message.content\n            solution = self.extract_code_from_response(raw_response)\n            \n            return solution, raw_response, elapsed\n            \n        except Exception as e:\n            logger.error(f\"Error solving problem with {model}: {e}\")\n            return \"\", f\"Error: {str(e)}\", 0.0\n    \n    def evaluate_solution(self, solution: str, problem_data: Dict) -> Tuple[bool, str, List[Dict]]:\n        \"\"\"\n        Evaluate a solution against test cases from LiveCodeBench.\n        \n        Args:\n            solution: Generated solution code\n            problem_data: Problem data including test cases\n            \n        Returns:\n            Tuple of (success, error_message, test_results)\n        \"\"\"\n        if not solution:\n            return False, \"No solution provided\", []\n        \n        test_results = []\n        \n        try:\n            # Get the perturbed test cases\n            public_tests = problem_data.get('perturbed_public_test_cases', [])\n            private_tests = problem_data.get('perturbed_private_test_cases', [])\n            \n            # Fallback to original test cases if no perturbed ones\n            if not public_tests and not private_tests:\n                public_tests = self.parse_test_cases(problem_data.get('public_test_cases', '[]'))\n                private_tests = self.parse_test_cases(problem_data.get('private_test_cases', '[]'))\n            \n            all_tests = public_tests + private_tests\n            \n            if not all_tests:\n                logger.warning(\"No test cases found in problem data\")\n                return False, \"No test cases found\", []\n            \n            # Determine test type\n            is_stdin_problem = False\n            if all_tests and isinstance(all_tests[0], dict):\n                is_stdin_problem = all_tests[0].get('testtype') == 'stdin'\n            \n            # Create temporary file to run the solution\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n                temp_file = f.name\n            \n            passed_count = 0\n            \n            for i, test in enumerate(all_tests):\n                if not isinstance(test, dict):\n                    test_results.append({'test': i, 'result': 'SKIP', 'error': 'Invalid test format'})\n                    continue\n                \n                test_input = test.get('input', '')\n                expected_output = test.get('output', '')\n                \n                try:\n                    if is_stdin_problem:\n                        # Write the solution and run with stdin\n                        with open(temp_file, 'w') as f:\n                            f.write(solution)\n                        \n                        # Run with the input\n                        result = subprocess.run(\n                            ['python', temp_file],\n                            input=test_input,\n                            capture_output=True,\n                            text=True,\n                            timeout=5\n                        )\n                        \n                        actual_output = result.stdout.strip()\n                        \n                    else:\n                        # Function-based problem\n                        # Try to extract function name from solution\n                        func_match = re.search(r'def\\s+(\\w+)\\s*\\(', solution)\n                        if not func_match:\n                            test_results.append({'test': i, 'result': 'FAIL', 'error': 'No function found'})\n                            continue\n                        \n                        func_name = func_match.group(1)\n                        \n                        # Write solution with test code\n                        with open(temp_file, 'w') as f:\n                            f.write(solution)\n                            f.write(f'\\n\\n# Test execution\\n')\n                            f.write(f'import json\\n')\n                            f.write(f'try:\\n')\n                            f.write(f'    result = {func_name}({test_input})\\n')\n                            f.write(f'    print(json.dumps(result))\\n')\n                            f.write(f'except Exception as e:\\n')\n                            f.write(f'    print(f\"ERROR: {{e}}\")\\n')\n                        \n                        result = subprocess.run(\n                            ['python', temp_file],\n                            capture_output=True,\n                            text=True,\n                            timeout=5\n                        )\n                        \n                        actual_output = result.stdout.strip()\n                        \n                        # Try to parse JSON output for function results\n                        if actual_output and not actual_output.startswith(\"ERROR\"):\n                            try:\n                                actual_output = json.loads(actual_output)\n                                actual_output = str(actual_output)\n                            except:\n                                pass\n                    \n                    # Compare outputs\n                    if actual_output == expected_output:\n                        test_results.append({'test': i, 'result': 'PASS'})\n                        passed_count += 1\n                    else:\n                        test_results.append({\n                            'test': i, \n                            'result': 'FAIL',\n                            'expected': expected_output[:100],\n                            'actual': actual_output[:100],\n                            'error': result.stderr[:200] if result.stderr else None\n                        })\n                        \n                except subprocess.TimeoutExpired:\n                    test_results.append({'test': i, 'result': 'TIMEOUT'})\n                except Exception as e:\n                    test_results.append({'test': i, 'result': 'ERROR', 'error': str(e)[:200]})\n            \n            # Clean up temp file\n            try:\n                os.unlink(temp_file)\n            except:\n                pass\n            \n            success = passed_count == len(all_tests)\n            return success, f\"{passed_count}/{len(all_tests)} tests passed\", test_results\n            \n        except Exception as e:\n            logger.error(f\"Evaluation error: {e}\\n{traceback.format_exc()}\")\n            return False, f\"Evaluation error: {str(e)}\", []\n    \n    def run_evaluation(self, models: List[str] = ['gpt-4', 'gpt-4o'], \n                       sample_size: Optional[int] = None):\n        \"\"\"\n        Run the complete evaluation pipeline.\n        \n        Args:\n            models: List of models to evaluate\n            sample_size: Optional limit on number of problems to evaluate\n        \"\"\"\n        # Load dataset if not already loaded\n        if self.dataset is None:\n            self.load_livecodebench()\n        \n        # Perturb problems if not already done\n        if not self.perturbed_problems:\n            self.perturb_all_problems()\n        \n        # Evaluate each model on perturbed problems\n        for model in models:\n            logger.info(f\"Evaluating {model}...\")\n            \n            for problem in tqdm(self.perturbed_problems, desc=f\"Evaluating {model}\"):\n                # Get problem date and extract month-year\n                date_str = problem.get('question_date', problem.get('date', problem.get('created_at', '')))\n                \n                if date_str:\n                    try:\n                        if isinstance(date_str, str):\n                            problem_date = datetime.strptime(date_str[:10], '%Y-%m-%d')\n                        else:\n                            problem_date = datetime.fromtimestamp(date_str)\n                        month_key = problem_date.strftime('%Y-%m')\n                    except:\n                        month_key = '2023-12'\n                else:\n                    month_key = '2023-12'\n                \n                # Get problem ID\n                problem_id = problem.get('question_id', problem.get('id', problem.get('problem_id', 'unknown')))\n                \n                # Solve the perturbed problem\n                solution, raw_response, solve_time = self.solve_problem(\n                    problem['perturbed_problem'], \n                    model,\n                    problem\n                )\n                \n                # Evaluate the solution using perturbed test cases\n                is_correct, error_msg, test_results = self.evaluate_solution(solution, problem)\n                \n                # Store detailed results\n                detailed_result = {\n                    'problem_id': problem_id,\n                    'model': model,\n                    'month': month_key,\n                    'correct': is_correct,\n                    'solution': solution,\n                    'raw_response': raw_response,\n                    'error_message': error_msg,\n                    'test_results': test_results,\n                    'solve_time': solve_time,\n                    'original_problem_data': {\n                        'question_id': problem.get('question_id'),\n                        'question_date': problem.get('question_date'),\n                        'question_title': problem.get('question_title'),\n                        'question_content': problem.get('question_content', problem.get('problem', '')),\n                        'starter_code': problem.get('starter_code'),\n                        'entry_point': problem.get('entry_point'),\n                        'public_test_cases': problem.get('public_test_cases', '[]'),\n                        'private_test_cases': problem.get('private_test_cases', '[]'),\n                        'difficulty': problem.get('difficulty'),\n                        'metadata': problem.get('metadata', '{}')\n                    },\n                    'perturbed_problem': problem['perturbed_problem'],\n                    'perturbed_public_test_cases': problem.get('perturbed_public_test_cases', []),\n                    'perturbed_private_test_cases': problem.get('perturbed_private_test_cases', []),\n                    'perturbation_time': problem.get('perturbation_time'),\n                    'perturbation_error': problem.get('perturbation_error')\n                }\n                \n                self.detailed_results.append(detailed_result)\n                \n                # Store summary results\n                self.results[model][month_key].append({\n                    'problem_id': problem_id,\n                    'correct': is_correct\n                })\n                \n                # Rate limiting between API calls\n                time.sleep(1)\n    \n    def calculate_accuracy(self) -> pd.DataFrame:\n        \"\"\"\n        Calculate accuracy for each model by month.\n        \n        Returns:\n            DataFrame with accuracy metrics\n        \"\"\"\n        accuracy_data = []\n        \n        for model in self.results:\n            for month in sorted(self.results[model].keys()):\n                month_results = self.results[model][month]\n                if month_results:\n                    accuracy = sum(r['correct'] for r in month_results) / len(month_results)\n                    accuracy_data.append({\n                        'model': model,\n                        'month': month,\n                        'accuracy': accuracy * 100,  # Convert to percentage\n                        'total_problems': len(month_results),\n                        'correct_problems': sum(r['correct'] for r in month_results)\n                    })\n        \n        df = pd.DataFrame(accuracy_data)\n        if not df.empty:\n            df['month'] = pd.to_datetime(df['month'] + '-01')\n            df = df.sort_values(['model', 'month'])\n        \n        return df\n    \n    def save_results(self, filepath: str = 'evaluation_results.json'):\n        \"\"\"\n        Save complete evaluation results including all intermediate outputs.\n        \n        Args:\n            filepath: Path to save results\n        \"\"\"\n        # Calculate summary\n        summary_df = self.calculate_accuracy()\n        \n        # Create problems collection with original and perturbed versions\n        problems_collection = []\n        unique_problems = {}\n        \n        # Collect unique problems from detailed results\n        for result in self.detailed_results:\n            problem_id = result['problem_id']\n            if problem_id not in unique_problems:\n                unique_problems[problem_id] = {\n                    'problem_id': problem_id,\n                    'original_problem': result['original_problem_data'],\n                    'perturbed_problem': result['perturbed_problem'],\n                    'perturbed_public_test_cases': result.get('perturbed_public_test_cases', []),\n                    'perturbed_private_test_cases': result.get('perturbed_private_test_cases', []),\n                    'perturbation_time': result.get('perturbation_time'),\n                    'perturbation_error': result.get('perturbation_error'),\n                    'solutions': {}\n                }\n            \n            # Add solution for this model\n            model = result['model']\n            unique_problems[problem_id]['solutions'][model] = {\n                'solution_code': result['solution'],\n                'raw_response': result['raw_response'],\n                'correct': result['correct'],\n                'error_message': result['error_message'],\n                'test_results': result['test_results'],\n                'solve_time': result['solve_time']\n            }\n        \n        problems_collection = list(unique_problems.values())\n        \n        results_to_save = {\n            'summary': summary_df.to_dict('records') if not summary_df.empty else [],\n            'problems': problems_collection,\n            'detailed_results': self.detailed_results,\n            'metadata': {\n                'evaluation_date': datetime.now().isoformat(),\n                'total_problems': len(self.perturbed_problems),\n                'models_evaluated': list(self.results.keys())\n            }\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_to_save, f, indent=2, default=str)\n        \n        logger.info(f\"Complete results saved to {filepath}\")\n        \n        # Also save a separate file with just problems and solutions for easy access\n        problems_and_solutions_file = 'problems_and_solutions.json'\n        with open(problems_and_solutions_file, 'w') as f:\n            json.dump({'problems': problems_collection}, f, indent=2, default=str)\n        logger.info(f\"Problems and solutions saved to {problems_and_solutions_file}\")\n        \n        # Also save a separate debugging file with just the failed cases\n        failed_cases = [r for r in self.detailed_results if not r['correct']]\n        if failed_cases:\n            with open('failed_cases.json', 'w') as f:\n                json.dump(failed_cases, f, indent=2, default=str)\n            logger.info(f\"Failed cases saved to failed_cases.json ({len(failed_cases)} failures)\")\n    \n    def plot_results(self):\n        \"\"\"Generate and display accuracy plots.\"\"\"\n        import matplotlib.pyplot as plt\n        \n        df = self.calculate_accuracy()\n        \n        if df.empty:\n            logger.warning(\"No results to plot\")\n            return\n        \n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n        \n        # Plot 1: Accuracy over time\n        for model in df['model'].unique():\n            model_data = df[df['model'] == model]\n            ax1.plot(model_data['month'], model_data['accuracy'], \n                    marker='o', label=model, linewidth=2, markersize=8)\n        \n        ax1.set_xlabel('Month')\n        ax1.set_ylabel('Accuracy (%)')\n        ax1.set_title('Model Accuracy on Perturbed LiveCodeBench Problems (May 2023 - Feb 2024)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_ylim(0, 100)\n        \n        # Format x-axis\n        import matplotlib.dates as mdates\n        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n        ax1.xaxis.set_major_locator(mdates.MonthLocator())\n        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n        \n        # Plot 2: Bar chart comparing total accuracy\n        model_names = df['model'].unique()\n        avg_accuracies = [df[df['model'] == m]['accuracy'].mean() for m in model_names]\n        \n        bars = ax2.bar(model_names, avg_accuracies, color=['blue', 'green'])\n        ax2.set_ylabel('Average Accuracy (%)')\n        ax2.set_title('Overall Average Accuracy Comparison')\n        ax2.set_ylim(0, 100)\n        \n        # Add value labels on bars\n        for bar, val in zip(bars, avg_accuracies):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{val:.1f}%', ha='center', va='bottom')\n        \n        ax2.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plt.savefig('accuracy_plot.png', dpi=150)\n        plt.show()\n        \n        logger.info(\"Plot saved as accuracy_plot.png\")\n\n\ndef main():\n    # Configuration\n    API_KEY = os.environ.get('OPENAI_API_KEY')\n    \n    if API_KEY == 'your-api-key-here':\n        logger.error(\"Please set your OpenAI API key in the OPENAI_API_KEY environment variable\")\n        return\n    \n    # Initialize evaluator\n    evaluator = LiveCodeBenchEvaluator(API_KEY)\n    \n    # Run evaluation\n    # Start with small sample for testing, then increase\n    evaluator.run_evaluation(\n        models=['gpt-4', 'gpt-4o'],\n        sample_size=5  # Start small for testing, then remove this limit\n    )\n    \n    # Calculate and display accuracy\n    accuracy_df = evaluator.calculate_accuracy()\n    print(\"\\n\" + \"=\"*60)\n    print(\"ACCURACY RESULTS BY MONTH\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            print(f\"\\n{model}:\")\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            for _, row in model_data.iterrows():\n                print(f\"  {row['month'].strftime('%b %Y')}: {row['accuracy']:.1f}% \"\n                      f\"({row['correct_problems']}/{row['total_problems']} correct)\")\n    \n    # Save complete results with debugging info\n    evaluator.save_results()\n    \n    # Plot results\n    try:\n        evaluator.plot_results()\n    except ImportError:\n        logger.warning(\"Matplotlib not available for plotting\")\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY STATISTICS\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            print(f\"\\n{model}:\")\n            print(f\"  Average accuracy: {model_data['accuracy'].mean():.1f}%\")\n            print(f\"  Min accuracy: {model_data['accuracy'].min():.1f}%\")\n            print(f\"  Max accuracy: {model_data['accuracy'].max():.1f}%\")\n            print(f\"  Total problems evaluated: {model_data['total_problems'].sum()}\")\n            print(f\"  Total correct: {model_data['correct_problems'].sum()}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-02T13:12:56.980523Z","iopub.execute_input":"2025-10-02T13:12:56.980881Z","execution_failed":"2025-10-02T13:27:45.536Z"}},"outputs":[{"name":"stderr","text":"Perturbing with O1:   7%|         | 27/400 [14:23<3:44:35, 36.13s/it]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import os\nimport json\nimport ast\nimport asyncio\nfrom datetime import datetime\nfrom collections import defaultdict\nfrom typing import List, Dict, Tuple, Optional, Any\nimport pandas as pd\nfrom tqdm import tqdm\nimport openai\nfrom datasets import load_dataset\nimport numpy as np\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nimport time\nimport logging\nimport subprocess\nimport tempfile\nimport re\nimport traceback\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\nclass LiveCodeBenchEvaluator:\n    def __init__(self, api_key: str):\n        \"\"\"\n        Initialize the evaluator with OpenAI API key.\n        \n        Args:\n            api_key: OpenAI API key\n        \"\"\"\n        self.client = openai.OpenAI(api_key=api_key)\n        self.dataset = None\n        self.perturbed_problems = []\n        self.results = defaultdict(lambda: defaultdict(list))\n        self.detailed_results = []  # Store everything for debugging\n        \n    def load_livecodebench(self):\n        \"\"\"Load the LiveCodeBench dataset.\"\"\"\n        try:\n            # Load LiveCodeBench dataset with correct path and version\n            # release_v1 contains problems from May 2023 to Mar 2024 (400 problems)\n            # which covers our target range\n            self.dataset = load_dataset(\n                \"livecodebench/code_generation_lite\", \n                version_tag=\"release_v1\",\n                trust_remote_code=True,\n                split=\"test\"\n            )\n            logger.info(f\"Loaded LiveCodeBench with {len(self.dataset)} problems\")\n            \n            # Filter for date range May 2023 - Feb 2024\n            filtered_data = []\n            for item in self.dataset:\n                # LiveCodeBench has 'question_date' field\n                date_field = item.get('question_date', item.get('date', item.get('created_at', '')))\n                if date_field:\n                    # Parse date and check range\n                    try:\n                        if isinstance(date_field, str):\n                            problem_date = datetime.strptime(date_field[:10], '%Y-%m-%d')\n                        else:\n                            # Might be timestamp\n                            problem_date = datetime.fromtimestamp(date_field)\n                            \n                        if datetime(2023, 5, 1) <= problem_date <= datetime(2024, 2, 29):\n                            filtered_data.append(item)\n                    except Exception as e:\n                        logger.debug(f\"Could not parse date {date_field}: {e}\")\n                        # Include if we can't parse date\n                        filtered_data.append(item)\n            \n            if not filtered_data:\n                # If no filtering worked, use all data\n                logger.warning(\"Date filtering didn't work, using all problems\")\n                filtered_data = list(self.dataset)\n            \n            self.dataset = filtered_data\n            logger.info(f\"Using {len(self.dataset)} problems\")\n            \n        except Exception as e:\n            logger.error(f\"Error loading LiveCodeBench: {e}\")\n            logger.info(\"Creating sample dataset for demonstration\")\n            self.dataset = self._create_sample_dataset()\n    \n    def _create_sample_dataset(self):\n        \"\"\"Create a sample dataset structure for demonstration if real dataset fails.\"\"\"\n        sample_problems = []\n        months = pd.date_range('2023-05', '2024-02', freq='MS')\n        \n        for i, month in enumerate(months):\n            for j in range(3):  # 3 problems per month for demo\n                sample_problems.append({\n                    'question_id': f'problem_{i}_{j}',\n                    'question_date': month.strftime('%Y-%m-%d'),\n                    'question_title': f'Two Sum Variation {i}_{j}',\n                    'question_content': f'''Given an array of integers nums and an integer target, \n                    return indices of the two numbers such that they add up to target.\n                    Example: nums = [{j}, {i}, {j+i}], target = {j+i}\n                    Output: [0, 1]''',\n                    'starter_code': 'def twoSum(nums, target):\\n    pass',\n                    'public_test_cases': json.dumps([\n                        {'input': f'[{j}, {i}, {j+i}], {j+i}', 'output': '[0, 1]', 'testtype': 'function'},\n                        {'input': f'[1, 2, 3], 5', 'output': '[1, 2]', 'testtype': 'function'}\n                    ]),\n                    'private_test_cases': json.dumps([\n                        {'input': '[2, 7, 11, 15], 9', 'output': '[0, 1]', 'testtype': 'function'}\n                    ]),\n                    'difficulty': 'easy',\n                    'metadata': json.dumps({'source': 'demo'})\n                })\n        \n        return sample_problems\n    \n    def parse_test_cases(self, test_case_str: str) -> List[Dict]:\n        \"\"\"Parse test cases from JSON string format.\"\"\"\n        if not test_case_str:\n            return []\n        \n        try:\n            if isinstance(test_case_str, str):\n                return json.loads(test_case_str)\n            elif isinstance(test_case_str, list):\n                return test_case_str\n            else:\n                return []\n        except (json.JSONDecodeError, TypeError) as e:\n            logger.warning(f\"Could not parse test cases: {e}\")\n            return []\n    \n    def extract_code_from_response(self, response: str) -> str:\n        \"\"\"Extract code from model response.\"\"\"\n        # Try to find code blocks\n        if '```python' in response:\n            matches = re.findall(r'```python\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        elif '```' in response:\n            matches = re.findall(r'```\\n(.*?)```', response, re.DOTALL)\n            if matches:\n                return matches[0].strip()\n        \n        # If no code blocks, return the whole response (it might be just code)\n        return response.strip()\n    \n    def perturb_problem(self, problem_data: Dict) -> Dict:\n        \"\"\"\n        Perturb a single problem using OpenAI's O1 model.\n        \n        Args:\n            problem_data: Original problem data\n            \n        Returns:\n            Problem data with perturbed version added\n        \"\"\"\n        try:\n            # Extract problem description\n            problem_text = problem_data.get('question_content', \n                                           problem_data.get('problem_description', \n                                           problem_data.get('problem', '')))\n            \n            # Parse test cases\n            public_tests = self.parse_test_cases(problem_data.get('public_test_cases', '[]'))\n            private_tests = self.parse_test_cases(problem_data.get('private_test_cases', '[]'))\n            \n            # Create a simpler test case representation for the prompt\n            test_examples = []\n            for test in public_tests: \n                if isinstance(test, dict):\n                    test_examples.append(f\"Input: {test.get('input', '')}\\nOutput: {test.get('output', '')}\")\n            \n            prompt = f\"\"\"You are tasked with creating a variation of the following programming problem.\n\nThe variation should:\n1. Keep the exact same algorithmic approach and complexity\n2. Change variable names, function names, and context (e.g., if it uses 'abc', use something like 'XYZ')\n3. Modify specific values in test cases consistently with the context change\n4. Maintain the same difficulty level and logic\n\nOriginal Problem:\n{problem_text}\n\nOriginal Test Examples:\n{chr(10).join(test_examples)}\n\nProvide the perturbed problem AND perturbed test cases in the following JSON format:\n{{\n    \"problem_statement\": \"...\",\n    \"test_cases\": [\n        {{\"input\": \"...\", \"output\": \"...\", \"testtype\": \"stdin\"}}\n    ]\n}}\n\nMake sure to perturb ALL test values consistently. If the original uses 'abc', 'acb', 'bac', etc., \nand you change to 'XYZ', then use 'XYZ', 'XZY', 'YXZ', etc. correspondingly.\"\"\"\n\n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=\"o1\",  # Using O1-preview model\n                messages=[\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                max_completion_tokens=100000  # O1 uses max_completion_tokens\n            )\n            \n            elapsed = time.time() - start_time\n            logger.info(f\"O1 perturbation took {elapsed:.1f} seconds\")\n            \n            response_text = response.choices[0].message.content.strip()\n            \n            # Try to parse the JSON response\n            try:\n                # Extract JSON from response if wrapped in code blocks\n                if '```json' in response_text:\n                    json_match = re.search(r'```json\\n(.*?)```', response_text, re.DOTALL)\n                    if json_match:\n                        response_text = json_match.group(1)\n                elif '```' in response_text:\n                    json_match = re.search(r'```\\n(.*?)```', response_text, re.DOTALL)\n                    if json_match:\n                        response_text = json_match.group(1)\n                \n                perturbed_data = json.loads(response_text)\n                \n                # Ensure we have the required fields\n                perturbed_problem = perturbed_data.get('problem_statement', response_text)\n                perturbed_test_cases = perturbed_data.get('test_cases', [])\n                \n                # Make sure we have enough perturbed test cases\n                # If not, create modified versions based on the pattern\n                total_tests_needed = len(public_tests) + len(private_tests)\n               # if len(perturbed_test_cases) < total_tests_needed:\n                #    logger.warning(f\"Only got {len(perturbed_test_cases)} perturbed tests, needed {total_tests_needed}\")\n                \n            except (json.JSONDecodeError, Exception) as e:\n                logger.warning(f\"Could not parse perturbed response as JSON: {e}\")\n                # Fall back to using the response as the problem statement\n                perturbed_problem = response_text\n                perturbed_test_cases = []\n                \n                # Try to create perturbed test cases by simple substitution\n                # This is a fallback - ideally O1 should provide them\n                if 'abc' in problem_text.lower() and any('XYZ' in s for s in [response_text]):\n                    # Simple character substitution for test cases\n                    mapping = {'a': 'X', 'b': 'Y', 'c': 'Z'}\n                    for test in public_tests + private_tests:\n                        if isinstance(test, dict):\n                            new_input = test.get('input', '')\n                            new_output = test.get('output', '')\n                            for old, new in mapping.items():\n                                new_input = new_input.replace(old, new)\n                                new_output = new_output.replace(old, new)\n                            perturbed_test_cases.append({\n                                'input': new_input,\n                                'output': new_output,\n                                'testtype': test.get('testtype', 'stdin')\n                            })\n            \n            # Split perturbed test cases back into public and private\n            num_public = len(public_tests)\n            perturbed_public = perturbed_test_cases[:num_public] if perturbed_test_cases else public_tests\n            perturbed_private = perturbed_test_cases[num_public:] if len(perturbed_test_cases) > num_public else private_tests\n            \n            return {\n                **problem_data,\n                'perturbed_problem': perturbed_problem,\n                'perturbed_public_test_cases': perturbed_public,\n                'perturbed_private_test_cases': perturbed_private,\n                'perturbation_time': elapsed\n            }\n            \n        except Exception as e:\n            logger.error(f\"Error perturbing problem: {e}\")\n            # Return original if perturbation fails\n            return {\n                **problem_data,\n                'perturbed_problem': problem_text,\n                'perturbed_public_test_cases': public_tests,\n                'perturbed_private_test_cases': private_tests,\n                'perturbation_error': str(e)\n            }\n    \n    def perturb_all_problems(self, sample_size: Optional[int] = None):\n        \"\"\"\n        Perturb all problems in the dataset using O1.\n        \n        Args:\n            sample_size: Optional limit on number of problems to perturb\n        \"\"\"\n        problems = self.dataset\n        if True:\n            problems = problems[348:]\n        \n        logger.info(f\"Perturbing {len(problems)} problems with O1...\")\n        \n        for problem in tqdm(problems, desc=\"Perturbing with O1\"):\n            perturbed = self.perturb_problem(problem)\n            self.perturbed_problems.append(perturbed)\n            \n            # O1 has strict rate limits, add delay\n            time.sleep(2)  # Adjust based on your rate limits\n    \n    def solve_problem(self, problem_text: str, model: str, original_problem_data: Dict) -> Tuple[str, str, float]:\n        \"\"\"\n        Solve a problem using specified GPT model.\n        \n        Args:\n            problem_text: Problem statement (perturbed)\n            model: Model name ('gpt-4' or 'gpt-4o')\n            original_problem_data: Original problem data for function signature\n            \n        Returns:\n            Tuple of (generated solution code, raw response, time taken)\n        \"\"\"\n        try:\n            # Check if this is a stdin/stdout problem or a function problem\n            public_tests = self.parse_test_cases(original_problem_data.get('public_test_cases', '[]'))\n            perturbed_tests = original_problem_data.get('perturbed_public_test_cases', public_tests)\n            \n            is_stdin_problem = False\n            if perturbed_tests and isinstance(perturbed_tests[0], dict):\n                is_stdin_problem = perturbed_tests[0].get('testtype') == 'stdin'\n            \n            # Build the prompt\n            prompt_parts = [\n                \"Solve the following programming problem.\",\n                \"Provide a complete, working Python solution.\",\n                \"\",\n                \"Problem:\",\n                problem_text,\n                \"\"\n            ]\n            \n            # Add guidance based on problem type\n            if is_stdin_problem:\n                prompt_parts.append(\"This is a standard competitive programming problem that reads from stdin and writes to stdout.\")\n                prompt_parts.append(\"Your solution should use input() to read data and print() to output results.\")\n            else:\n                # Extract function signature if available\n                starter_code = original_problem_data.get('starter_code', '')\n                entry_point = original_problem_data.get('entry_point', '')\n                \n                if starter_code:\n                    prompt_parts.append(f\"Use this function signature:\")\n                    prompt_parts.append(starter_code)\n                elif entry_point:\n                    prompt_parts.append(f\"The main function should be named: {entry_point}\")\n            \n            # Add example from perturbed test cases if available\n            if perturbed_tests:\n                prompt_parts.append(\"\\nExample:\")\n                test_example = perturbed_tests[0]\n                if isinstance(test_example, dict):\n                    prompt_parts.append(f\"Input: {test_example.get('input', '')}\")\n                    prompt_parts.append(f\"Output: {test_example.get('output', '')}\")\n            \n            prompt_parts.append(\"\")\n            prompt_parts.append(\"Provide the complete Python code solution. Do not include any explanations, just the code:\")\n            \n            # Join all parts with newline\n            prompt = '\\n'.join(prompt_parts)\n\n            # Map model names to actual OpenAI model identifiers\n            model_mapping = {\n                'gpt-4': 'gpt-4-turbo',  # Latest GPT-4 Turbo\n                'gpt-4o': 'gpt-4o'  # GPT-4o\n            }\n            \n            actual_model = model_mapping.get(model, model)\n            \n            start_time = time.time()\n            response = self.client.chat.completions.create(\n                model=actual_model,\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are an expert competitive programmer. Provide complete, efficient, and correct Python code solutions. Output only code, no explanations.\"},\n                    {\"role\": \"user\", \"content\": prompt}\n                ],\n                temperature=0.0,  # Use 0 for deterministic output\n                max_tokens=100000  # Much higher limit for complex solutions\n            )\n            \n            elapsed = time.time() - start_time\n            logger.debug(f\"{model} took {elapsed:.1f} seconds\")\n            \n            raw_response = response.choices[0].message.content\n            solution = self.extract_code_from_response(raw_response)\n            \n            return solution, raw_response, elapsed\n            \n        except Exception as e:\n            logger.error(f\"Error solving problem with {model}: {e}\")\n            return \"\", f\"Error: {str(e)}\", 0.0\n    \n    def evaluate_solution(self, solution: str, problem_data: Dict) -> Tuple[bool, str, List[Dict]]:\n        \"\"\"\n        Evaluate a solution against test cases from LiveCodeBench.\n        \n        Args:\n            solution: Generated solution code\n            problem_data: Problem data including test cases\n            \n        Returns:\n            Tuple of (success, error_message, test_results)\n        \"\"\"\n        if not solution:\n            return False, \"No solution provided\", []\n        \n        test_results = []\n        \n        try:\n            # Get the perturbed test cases\n            public_tests = problem_data.get('perturbed_public_test_cases', [])\n            private_tests = problem_data.get('perturbed_private_test_cases', [])\n            \n            # Fallback to original test cases if no perturbed ones\n            if not public_tests and not private_tests:\n                public_tests = self.parse_test_cases(problem_data.get('public_test_cases', '[]'))\n                private_tests = self.parse_test_cases(problem_data.get('private_test_cases', '[]'))\n            \n            all_tests = public_tests + private_tests\n            \n            if not all_tests:\n                logger.warning(\"No test cases found in problem data\")\n                return False, \"No test cases found\", []\n            \n            # Determine test type\n            is_stdin_problem = False\n            if all_tests and isinstance(all_tests[0], dict):\n                is_stdin_problem = all_tests[0].get('testtype') == 'stdin'\n            \n            # Create temporary file to run the solution\n            with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n                temp_file = f.name\n            \n            passed_count = 0\n            \n            for i, test in enumerate(all_tests):\n                if not isinstance(test, dict):\n                    test_results.append({'test': i, 'result': 'SKIP', 'error': 'Invalid test format'})\n                    continue\n                \n                test_input = test.get('input', '')\n                expected_output = test.get('output', '')\n                \n                try:\n                    if is_stdin_problem:\n                        # Write the solution and run with stdin\n                        with open(temp_file, 'w') as f:\n                            f.write(solution)\n                        \n                        # Run with the input\n                        result = subprocess.run(\n                            ['python', temp_file],\n                            input=test_input,\n                            capture_output=True,\n                            text=True,\n                            timeout=5\n                        )\n                        \n                        actual_output = result.stdout.strip()\n                        \n                    else:\n                        # Function-based problem\n                        # Try to extract function name from solution\n                        func_match = re.search(r'def\\s+(\\w+)\\s*\\(', solution)\n                        if not func_match:\n                            test_results.append({'test': i, 'result': 'FAIL', 'error': 'No function found'})\n                            continue\n                        \n                        func_name = func_match.group(1)\n                        \n                        # Write solution with test code\n                        with open(temp_file, 'w') as f:\n                            f.write(solution)\n                            f.write(f'\\n\\n# Test execution\\n')\n                            f.write(f'import json\\n')\n                            f.write(f'try:\\n')\n                            f.write(f'    result = {func_name}({test_input})\\n')\n                            f.write(f'    print(json.dumps(result))\\n')\n                            f.write(f'except Exception as e:\\n')\n                            f.write(f'    print(f\"ERROR: {{e}}\")\\n')\n                        \n                        result = subprocess.run(\n                            ['python', temp_file],\n                            capture_output=True,\n                            text=True,\n                            timeout=5\n                        )\n                        \n                        actual_output = result.stdout.strip()\n                        \n                        # Try to parse JSON output for function results\n                        if actual_output and not actual_output.startswith(\"ERROR\"):\n                            try:\n                                actual_output = json.loads(actual_output)\n                                actual_output = str(actual_output)\n                            except:\n                                pass\n                    \n                    # Compare outputs\n                    if actual_output == expected_output:\n                        test_results.append({'test': i, 'result': 'PASS'})\n                        passed_count += 1\n                    else:\n                        test_results.append({\n                            'test': i, \n                            'result': 'FAIL',\n                            'expected': expected_output[:100],\n                            'actual': actual_output[:100],\n                            'error': result.stderr[:200] if result.stderr else None\n                        })\n                        \n                except subprocess.TimeoutExpired:\n                    test_results.append({'test': i, 'result': 'TIMEOUT'})\n                except Exception as e:\n                    test_results.append({'test': i, 'result': 'ERROR', 'error': str(e)[:200]})\n            \n            # Clean up temp file\n            try:\n                os.unlink(temp_file)\n            except:\n                pass\n            \n            success = passed_count == len(all_tests)\n            return success, f\"{passed_count}/{len(all_tests)} tests passed\", test_results\n            \n        except Exception as e:\n            logger.error(f\"Evaluation error: {e}\\n{traceback.format_exc()}\")\n            return False, f\"Evaluation error: {str(e)}\", []\n    \n    def run_evaluation(self, models: List[str] = ['gpt-4', 'gpt-4o'], \n                       sample_size: Optional[int] = None):\n        \"\"\"\n        Run the complete evaluation pipeline.\n        \n        Args:\n            models: List of models to evaluate\n            sample_size: Optional limit on number of problems to evaluate\n        \"\"\"\n        # Load dataset if not already loaded\n        if self.dataset is None:\n            self.load_livecodebench()\n        \n        # Perturb problems if not already done\n        if not self.perturbed_problems:\n            self.perturb_all_problems()\n        \n        # Evaluate each model on perturbed problems\n        for model in models:\n            logger.info(f\"Evaluating {model}...\")\n            \n            for problem in tqdm(self.perturbed_problems, desc=f\"Evaluating {model}\"):\n                # Get problem date and extract month-year\n                date_str = problem.get('question_date', problem.get('date', problem.get('created_at', '')))\n                \n                if date_str:\n                    try:\n                        if isinstance(date_str, str):\n                            problem_date = datetime.strptime(date_str[:10], '%Y-%m-%d')\n                        else:\n                            problem_date = datetime.fromtimestamp(date_str)\n                        month_key = problem_date.strftime('%Y-%m')\n                    except:\n                        month_key = '2023-12'\n                else:\n                    month_key = '2023-12'\n                \n                # Get problem ID\n                problem_id = problem.get('question_id', problem.get('id', problem.get('problem_id', 'unknown')))\n                \n                # Solve the perturbed problem\n                solution, raw_response, solve_time = self.solve_problem(\n                    problem['perturbed_problem'], \n                    model,\n                    problem\n                )\n                \n                # Evaluate the solution using perturbed test cases\n                is_correct, error_msg, test_results = self.evaluate_solution(solution, problem)\n                \n                # Store detailed results\n                detailed_result = {\n                    'problem_id': problem_id,\n                    'model': model,\n                    'month': month_key,\n                    'correct': is_correct,\n                    'solution': solution,\n                    'raw_response': raw_response,\n                    'error_message': error_msg,\n                    'test_results': test_results,\n                    'solve_time': solve_time,\n                    'original_problem_data': {\n                        'question_id': problem.get('question_id'),\n                        'question_date': problem.get('question_date'),\n                        'question_title': problem.get('question_title'),\n                        'question_content': problem.get('question_content', problem.get('problem', '')),\n                        'starter_code': problem.get('starter_code'),\n                        'entry_point': problem.get('entry_point'),\n                        'public_test_cases': problem.get('public_test_cases', '[]'),\n                        'private_test_cases': problem.get('private_test_cases', '[]'),\n                        'difficulty': problem.get('difficulty'),\n                        'metadata': problem.get('metadata', '{}')\n                    },\n                    'perturbed_problem': problem['perturbed_problem'],\n                    'perturbed_public_test_cases': problem.get('perturbed_public_test_cases', []),\n                    'perturbed_private_test_cases': problem.get('perturbed_private_test_cases', []),\n                    'perturbation_time': problem.get('perturbation_time'),\n                    'perturbation_error': problem.get('perturbation_error')\n                }\n                \n                self.detailed_results.append(detailed_result)\n                \n                # Store summary results\n                self.results[model][month_key].append({\n                    'problem_id': problem_id,\n                    'correct': is_correct\n                })\n                \n                # Rate limiting between API calls\n                time.sleep(1)\n    \n    def calculate_accuracy(self) -> pd.DataFrame:\n        \"\"\"\n        Calculate accuracy for each model by month.\n        \n        Returns:\n            DataFrame with accuracy metrics\n        \"\"\"\n        accuracy_data = []\n        \n        for model in self.results:\n            for month in sorted(self.results[model].keys()):\n                month_results = self.results[model][month]\n                if month_results:\n                    accuracy = sum(r['correct'] for r in month_results) / len(month_results)\n                    accuracy_data.append({\n                        'model': model,\n                        'month': month,\n                        'accuracy': accuracy * 100,  # Convert to percentage\n                        'total_problems': len(month_results),\n                        'correct_problems': sum(r['correct'] for r in month_results)\n                    })\n        \n        df = pd.DataFrame(accuracy_data)\n        if not df.empty:\n            df['month'] = pd.to_datetime(df['month'] + '-01')\n            df = df.sort_values(['model', 'month'])\n        \n        return df\n    \n    def save_results(self, filepath: str = 'evaluation_results.json'):\n        \"\"\"\n        Save complete evaluation results including all intermediate outputs.\n        \n        Args:\n            filepath: Path to save results\n        \"\"\"\n        # Calculate summary\n        summary_df = self.calculate_accuracy()\n        \n        # Create problems collection with original and perturbed versions\n        problems_collection = []\n        unique_problems = {}\n        \n        # Collect unique problems from detailed results\n        for result in self.detailed_results:\n            problem_id = result['problem_id']\n            if problem_id not in unique_problems:\n                unique_problems[problem_id] = {\n                    'problem_id': problem_id,\n                    'original_problem': result['original_problem_data'],\n                    'perturbed_problem': result['perturbed_problem'],\n                    'perturbed_public_test_cases': result.get('perturbed_public_test_cases', []),\n                    'perturbed_private_test_cases': result.get('perturbed_private_test_cases', []),\n                    'perturbation_time': result.get('perturbation_time'),\n                    'perturbation_error': result.get('perturbation_error'),\n                    'solutions': {}\n                }\n            \n            # Add solution for this model\n            model = result['model']\n            unique_problems[problem_id]['solutions'][model] = {\n                'solution_code': result['solution'],\n                'raw_response': result['raw_response'],\n                'correct': result['correct'],\n                'error_message': result['error_message'],\n                'test_results': result['test_results'],\n                'solve_time': result['solve_time']\n            }\n        \n        problems_collection = list(unique_problems.values())\n        \n        results_to_save = {\n            'summary': summary_df.to_dict('records') if not summary_df.empty else [],\n            'problems': problems_collection,\n            'detailed_results': self.detailed_results,\n            'metadata': {\n                'evaluation_date': datetime.now().isoformat(),\n                'total_problems': len(self.perturbed_problems),\n                'models_evaluated': list(self.results.keys())\n            }\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(results_to_save, f, indent=2, default=str)\n        \n        logger.info(f\"Complete results saved to {filepath}\")\n        \n        # Also save a separate file with just problems and solutions for easy access\n        problems_and_solutions_file = 'problems_and_solutions.json'\n        with open(problems_and_solutions_file, 'w') as f:\n            json.dump({'problems': problems_collection}, f, indent=2, default=str)\n        logger.info(f\"Problems and solutions saved to {problems_and_solutions_file}\")\n        \n        # Also save a separate debugging file with just the failed cases\n        failed_cases = [r for r in self.detailed_results if not r['correct']]\n        if failed_cases:\n            with open('failed_cases.json', 'w') as f:\n                json.dump(failed_cases, f, indent=2, default=str)\n            logger.info(f\"Failed cases saved to failed_cases.json ({len(failed_cases)} failures)\")\n    \n    def plot_results(self):\n        \"\"\"Generate and display accuracy plots.\"\"\"\n        import matplotlib.pyplot as plt\n        \n        df = self.calculate_accuracy()\n        \n        if df.empty:\n            logger.warning(\"No results to plot\")\n            return\n        \n        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n        \n        # Plot 1: Accuracy over time\n        for model in df['model'].unique():\n            model_data = df[df['model'] == model]\n            ax1.plot(model_data['month'], model_data['accuracy'], \n                    marker='o', label=model, linewidth=2, markersize=8)\n        \n        ax1.set_xlabel('Month')\n        ax1.set_ylabel('Accuracy (%)')\n        ax1.set_title('Model Accuracy on Perturbed LiveCodeBench Problems (May 2023 - Feb 2024)')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax1.set_ylim(0, 100)\n        \n        # Format x-axis\n        import matplotlib.dates as mdates\n        ax1.xaxis.set_major_formatter(mdates.DateFormatter('%b %Y'))\n        ax1.xaxis.set_major_locator(mdates.MonthLocator())\n        plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45)\n        \n        # Plot 2: Bar chart comparing total accuracy\n        model_names = df['model'].unique()\n        avg_accuracies = [df[df['model'] == m]['accuracy'].mean() for m in model_names]\n        \n        bars = ax2.bar(model_names, avg_accuracies, color=['blue', 'green'])\n        ax2.set_ylabel('Average Accuracy (%)')\n        ax2.set_title('Overall Average Accuracy Comparison')\n        ax2.set_ylim(0, 100)\n        \n        # Add value labels on bars\n        for bar, val in zip(bars, avg_accuracies):\n            ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{val:.1f}%', ha='center', va='bottom')\n        \n        ax2.grid(True, alpha=0.3, axis='y')\n        \n        plt.tight_layout()\n        plt.savefig('accuracy_plot.png', dpi=150)\n        plt.show()\n        \n        logger.info(\"Plot saved as accuracy_plot.png\")\n\n\ndef main():\n    # Configuration\n    API_KEY = os.environ.get('OPENAI_API_KEY')\n    \n    if API_KEY == 'your-api-key-here':\n        logger.error(\"Please set your OpenAI API key in the OPENAI_API_KEY environment variable\")\n        return\n    \n    # Initialize evaluator\n    evaluator = LiveCodeBenchEvaluator(API_KEY)\n    \n    # Run evaluation\n    # Start with small sample for testing, then increase\n    evaluator.run_evaluation(\n        models=['gpt-4', 'gpt-4o'],\n        sample_size=5  # Start small for testing, then remove this limit\n    )\n    \n    # Calculate and display accuracy\n    accuracy_df = evaluator.calculate_accuracy()\n    print(\"\\n\" + \"=\"*60)\n    print(\"ACCURACY RESULTS BY MONTH\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            print(f\"\\n{model}:\")\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            for _, row in model_data.iterrows():\n                print(f\"  {row['month'].strftime('%b %Y')}: {row['accuracy']:.1f}% \"\n                      f\"({row['correct_problems']}/{row['total_problems']} correct)\")\n    \n    # Save complete results with debugging info\n    evaluator.save_results()\n    \n    # Plot results\n    try:\n        evaluator.plot_results()\n    except ImportError:\n        logger.warning(\"Matplotlib not available for plotting\")\n    \n    # Print summary statistics\n    print(\"\\n\" + \"=\"*60)\n    print(\"SUMMARY STATISTICS\")\n    print(\"=\"*60)\n    if not accuracy_df.empty:\n        for model in accuracy_df['model'].unique():\n            model_data = accuracy_df[accuracy_df['model'] == model]\n            print(f\"\\n{model}:\")\n            print(f\"  Average accuracy: {model_data['accuracy'].mean():.1f}%\")\n            print(f\"  Min accuracy: {model_data['accuracy'].min():.1f}%\")\n            print(f\"  Max accuracy: {model_data['accuracy'].max():.1f}%\")\n            print(f\"  Total problems evaluated: {model_data['total_problems'].sum()}\")\n            print(f\"  Total correct: {model_data['correct_problems'].sum()}\")\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport numpy as np\nfrom datetime import datetime\nfrom collections import defaultdict\n\ndef analyze_json_results(filepath='evaluation_results.json'):\n    \"\"\"\n    Analyze the JSON file to extract per-month accuracy for both GPT-4 and GPT-4o.\n    \n    Args:\n        filepath: Path to the JSON results file\n        \n    Returns:\n        dict: Contains arrays of accuracy values for each model\n    \"\"\"\n    # Load the JSON file\n    with open(filepath, 'r') as f:\n        data = json.load(f)\n    \n    # Initialize result structure\n    results = {\n        'gpt-4': {},\n        'gpt-4o': {}\n    }\n    \n    # Extract accuracy from summary\n    if 'summary' in data:\n        for record in data['summary']:\n            model = record['model']\n            month = record['month']\n            accuracy = record['accuracy']\n            \n            # Convert month string to datetime for proper sorting\n            if isinstance(month, str):\n                # Handle different date formats\n                try:\n                    # Try parsing as full datetime\n                    month_dt = datetime.strptime(month[:7], '%Y-%m')\n                except:\n                    try:\n                        # Try parsing if already in YYYY-MM format\n                        month_dt = datetime.strptime(month, '%Y-%m')\n                    except:\n                        # Skip if can't parse\n                        continue\n                \n                month_key = month_dt.strftime('%Y-%m')\n                \n                if model in results:\n                    results[model][month_key] = accuracy\n    \n    # Alternative: Calculate from detailed_results if summary is empty\n    if not any(results[m] for m in results):\n        print(\"Summary empty, calculating from detailed results...\")\n        \n        monthly_stats = defaultdict(lambda: defaultdict(lambda: {'correct': 0, 'total': 0}))\n        \n        for result in data.get('detailed_results', []):\n            model = result['model']\n            month = result['month']\n            is_correct = result['correct']\n            \n            if model in results:\n                monthly_stats[model][month]['total'] += 1\n                if is_correct:\n                    monthly_stats[model][month]['correct'] += 1\n        \n        # Calculate accuracies\n        for model in monthly_stats:\n            for month in monthly_stats[model]:\n                stats = monthly_stats[model][month]\n                if stats['total'] > 0:\n                    accuracy = (stats['correct'] / stats['total']) * 100\n                    results[model][month] = accuracy\n    \n    # Sort by month and create arrays\n    months_order = ['2023-05', '2023-06', '2023-07', '2023-08', '2023-09', \n                    '2023-10', '2023-11', '2023-12', '2024-01', '2024-02']\n    \n    gpt4_accuracies = []\n    gpt4o_accuracies = []\n    \n    for month in months_order:\n        # Get accuracy for each model, default to 0 if not found\n        gpt4_accuracies.append(results['gpt-4'].get(month, 0))\n        gpt4o_accuracies.append(results['gpt-4o'].get(month, 0))\n    \n    # Convert to numpy arrays\n    gpt4_array = np.array(gpt4_accuracies)\n    gpt4o_array = np.array(gpt4o_accuracies)\n    \n    # Print results for verification\n    print(\"=\"*60)\n    print(\"EXTRACTED ACCURACY DATA\")\n    print(\"=\"*60)\n    print(\"\\nGPT-4 Accuracies by month:\")\n    for i, month in enumerate(months_order):\n        if gpt4_accuracies[i] > 0:\n            print(f\"  {month}: {gpt4_accuracies[i]:.2f}%\")\n    \n    print(\"\\nGPT-4o Accuracies by month:\")\n    for i, month in enumerate(months_order):\n        if gpt4o_accuracies[i] > 0:\n            print(f\"  {month}: {gpt4o_accuracies[i]:.2f}%\")\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"Arrays ready for plotting:\")\n    print(f\"gpt4_percentages = {gpt4_array}\")\n    print(f\"gpt4o_percentages = {gpt4o_array}\")\n    print(\"=\"*60)\n    \n    return {\n        'gpt4': gpt4_array,\n        'gpt4o': gpt4o_array,\n        'months': months_order\n    }\n\n\nif __name__ == \"__main__\":\n    # Analyze the JSON file and extract accuracy arrays\n    print(\"Analyzing evaluation results from 'evaluation_results.json'...\")\n    \n    accuracy_data = analyze_json_results('evaluation_results.json')\n    \n    # Store the arrays in variables that can be used by the plotting code\n    gpt4_percentages = accuracy_data['gpt4']\n    gpt4o_percentages = accuracy_data['gpt4o']\n    \n    print(\"\\n Accuracy arrays extracted successfully!\")\n    print(\"Variables 'gpt4_percentages' and 'gpt4o_percentages' are now available for plotting.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\n\n\ndef plot_accuracy_comparison(gpt4_percentages, gpt4o_percentages):\n    \"\"\"\n    Plot the accuracy comparison using the extracted arrays.\n    \n    Args:\n        gpt4_percentages: numpy array of GPT-4 accuracies\n        gpt4o_percentages: numpy array of GPT-4o accuracies\n    \"\"\"\n    # Month labels for x-axis\n    months = ['May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec', 'Jan', 'Feb']\n    x = np.arange(len(months))\n    \n    # Create the plot\n    fig, ax = plt.subplots(figsize=(12, 7))\n    \n    # Plot the lines - matching the original style\n    ax.plot(x, gpt4_percentages, color='#ff7f0e', linewidth=2.5, label='GPT-4')\n    ax.plot(x, gpt4o_percentages, color='#d62728', linewidth=2.5, label='GPT-4o')\n    \n    # Add vertical dashed lines for knowledge cutoffs\n    ax.axvline(x=6, color='#d62728', linestyle='--', linewidth=1.5, alpha=0.7)  # November - GPT-4o color\n    ax.axvline(x=7, color='#ff7f0e', linestyle='--', linewidth=1.5, alpha=0.7)  # December - GPT-4 color\n    \n    # Styling to match the reference image\n    ax.set_ylim(0, 100)\n    ax.set_xlim(-0.5, 9.5)\n    \n    # Set labels\n    ax.set_ylabel('Pass@1', fontsize=20)\n    ax.set_title('Accuracy on perturbed LiveCodeBench', fontsize=20, pad=20)\n    \n    # Set x-axis ticks and labels\n    ax.set_xticks(x)\n    ax.set_xticklabels(months, fontsize=16)\n    \n    # Set y-axis ticks\n    ax.set_yticks(np.arange(0, 101, 20))\n    ax.set_yticklabels(np.arange(0, 101, 20), fontsize=14)\n    \n    # Add grid with light style\n    ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n    ax.set_axisbelow(True)\n    \n    # Add legend\n    ax.legend(loc='upper left', fontsize=12, frameon=True, fancybox=False, \n              edgecolor='black', framealpha=1, borderpad=1)\n    \n    # Adjust layout\n    plt.tight_layout()\n    \n    # Save the figure\n    plt.savefig('livecodebench_accuracy_plot.png', dpi=150, bbox_inches='tight')\n    \n    # Display the plot\n    plt.show()\n    \n    # Print the values for reference\n    print(\"\\n\" + \"=\"*60)\n    print(\"PLOTTED VALUES\")\n    print(\"=\"*60)\n    print(\"GPT-4 percentages:\", gpt4_percentages)\n    print(\"GPT-4o percentages:\", gpt4o_percentages)\n    \n    # Calculate and print statistics\n    gpt4_valid = gpt4_percentages[gpt4_percentages > 0]\n    gpt4o_valid = gpt4o_percentages[gpt4o_percentages > 0]\n    \n    if len(gpt4_valid) > 0:\n        print(f\"\\nGPT-4 Statistics:\")\n        print(f\"  Average: {np.mean(gpt4_valid):.2f}%\")\n        print(f\"  Min: {np.min(gpt4_valid):.2f}%\")\n        print(f\"  Max: {np.max(gpt4_valid):.2f}%\")\n    \n    if len(gpt4o_valid) > 0:\n        print(f\"\\nGPT-4o Statistics:\")\n        print(f\"  Average: {np.mean(gpt4o_valid):.2f}%\")\n        print(f\"  Min: {np.min(gpt4o_valid):.2f}%\")\n        print(f\"  Max: {np.max(gpt4o_valid):.2f}%\")\n    \n    print(\"=\"*60)\n    \n    return fig, ax\n\n\nif __name__ == \"__main__\":\n    # Check if arrays exist in the current namespace\n    try:\n        # These variables should be set by running the analysis code first\n        print(\"Using accuracy arrays from analysis...\")\n        fig, ax = plot_accuracy_comparison(gpt4_percentages, gpt4o_percentages)\n        print(\"\\n Plot saved as 'livecodebench_accuracy_plot.png'\")\n        \n    except NameError:\n        print(\"ERROR: Accuracy arrays not found!\")\n        print(\"Please run the JSON analysis code first to extract the arrays.\")\n        print(\"\\nAlternatively, you can manually set the arrays like this:\")\n        print(\"  gpt4_percentages = np.array([...])\")\n        print(\"  gpt4o_percentages = np.array([...])\")\n        print(\"Then run: plot_accuracy_comparison(gpt4_percentages, gpt4o_percentages)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}